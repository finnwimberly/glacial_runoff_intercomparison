{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25378f22-1058-490c-a50c-2162574d47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "fpath1 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/Precipitation Analysis/precip_components_past'\n",
    "regions = {'RGI01':'alaska', 'RGI02':'westerncanada', 'RGI06':'iceland', 'RGI08':'scandinavia', \n",
    "           'RGI10':['East', 'North', 'Altay', 'Chukotka'], 'RGI11':'centraleurope', 'RGI12':'caucasus', 'RGI13':'centralasiaN', \n",
    "           'RGI14': 'centralasiaW', 'RGI15':'centralasiaS', 'RGI16':'Andes', 'RGI17':'southernandes',\n",
    "           'RGI18':'newzealand'}\n",
    "\n",
    "fpath2 = 'PAST/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92fe1a86-e750-485a-b790-44b9cddec034",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = {'RHINE':'6242', 'RHONE':'6243','PO':'6241', 'DANUBE':'6202', 'TITICACA':'3912', 'SANTA':'3425', \n",
    "            'OCONA':'3418', 'MAJES':'3416', 'MAGDALENA':'3227', 'AMAZON':'3203', 'YELCHO':'3429', \n",
    "            'VALDIVIA':'3428', 'SERRANO':'3426', 'RAPEL':'3423', 'PUELO':'3422', 'PASCUA':'3420', \n",
    "            'PALENA':'3419', 'HUASCO':'3412', 'COPIAPO':'3409', 'CISNES':'3408', 'BIOBIO':'3405', 'BAKER':'3404',\n",
    "            'AZOPARDO':'3403', 'AISEN':'3401', 'SANTA CRUZ':'3244', 'NEGRO':'3232', 'COLORADO':'3212', \n",
    "            'CHICO':'3209', 'TORNEALVEN':'6255', 'THJORSA':'6254', 'OLFUSA':'6237', 'LULEALVEN':'6227', \n",
    "            'KUBAN':'6223', 'KALIXALVEN':'6219', 'GLOMAA':'6213', 'DRAMSELVA':'6209', 'SVARTA':'6110', \n",
    "            'LAGARFLJOT':'6104', 'JOKULSA A FJOLLUM':'6101', 'CLUTHA':'5406', 'YUKON':'4435', 'TAKU':'4431', \n",
    "             'SUSITNA':'4430','STIKINE':'4428', 'SKEENA':'4427','SKAGIT':'4426','NUSHAGAK':'4418','NASS':'4416',\n",
    "            'KUSKOKWIM':'4414','FRASER':'4410', 'COPPER':'4408', 'COLUMBIA':'4406', 'ALSEK':'4401', 'NELSON':'4125', \n",
    "              'MACKENZIE':'4123','COLVILLE':'4110', 'YSYK-KOL':'2919', 'UVS NUUR':'2918', 'TARIM HE':'2914', \n",
    "              'TALAS':'2913', 'LAKE BALKHASH':'2910','HAR US NUUR':'2909', 'CHUY':'2905', 'ARAL SEA':'2902', \n",
    "              'YELLOW RIVER':'2434', 'MEKONG':'2421', 'KAMCHATKA':'2413', 'SALWEEN':'2319', 'IRRAWADDY':'2310', \n",
    "              'INDUS':'2309', 'GANGES':'2306','BRAHMAPUTRA':'2302', 'OB':'2108', 'INDIGIRKA':'2103','YANGTZE' : '2433'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25a0df-1620-4f97-91f8-154e20783daa",
   "metadata": {},
   "source": [
    "#### Loading in solid precip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "463bd5f0-e78c-46c3-9849-2a864e573484",
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_solid_prcp = {}\n",
    "\n",
    "for RGI_region, region_name in regions.items():\n",
    "    if isinstance(region_name, list):  # Check if region_name is a list\n",
    "        combined_data = None\n",
    "        for subregion in region_name:\n",
    "            file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{subregion}_Accumulation_sfc_r1.dat\")\n",
    "            temp_df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "            temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "            temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "            if combined_data is None:\n",
    "                combined_data = temp_df\n",
    "            else:\n",
    "                combined_data = combined_data.add(temp_df, fill_value=0)  # Add with proper alignment and fill missing values with 0\n",
    "        regional_solid_prcp[RGI_region] = combined_data\n",
    "    else:\n",
    "        file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{region_name}_Accumulation_sfc_r1.dat\")\n",
    "        temp_df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "        regional_solid_prcp[RGI_region] = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b145de6-a8ab-4261-94ba-f09040a55e3a",
   "metadata": {},
   "source": [
    "#### Loading in liquid precip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69335e7e-75b2-420c-96df-af36f12867ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_liq_prcp = {}\n",
    "\n",
    "for RGI_region, region_name in regions.items():\n",
    "    if isinstance(region_name, list):  # Check if region_name is a list\n",
    "        combined_data = None\n",
    "        for subregion in region_name:\n",
    "            file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{subregion}_Rain_sfc_r1.dat\")\n",
    "            temp_df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "            temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "            temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "            if combined_data is None:\n",
    "                combined_data = temp_df\n",
    "            else:\n",
    "                combined_data = combined_data.add(temp_df, fill_value=0)  # Add with proper alignment and fill missing values with 0\n",
    "        regional_liq_prcp[RGI_region] = combined_data\n",
    "    else:\n",
    "        file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{region_name}_Rain_sfc_r1.dat\")\n",
    "        temp_df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "        regional_liq_prcp[RGI_region] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e80225d0-5097-40ea-88e5-78243a7463c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing area data\n",
    "areas = {}\n",
    "\n",
    "for RGI_region, region_name in regions.items():\n",
    "    if isinstance(region_name, list):  # Check if region_name is a list\n",
    "        combined_data = None\n",
    "        for subregion in region_name:\n",
    "            file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{subregion}_Area_r1.dat\")\n",
    "            temp_df = pd.read_csv(file_path, sep='\\s+', index_col=\"ID\")\n",
    "            temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "            temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "            temp_df = temp_df.mul(1e6)  # Convert km^2 to m^2\n",
    "            if combined_data is None:\n",
    "                combined_data = temp_df\n",
    "            else:\n",
    "                combined_data = combined_data.add(temp_df, fill_value=0)  # Add with proper alignment and fill missing values with 0\n",
    "        areas[RGI_region] = combined_data\n",
    "    else:\n",
    "        file_path = os.path.join(fpath1, RGI_region, fpath2, f\"{region_name}_Area_r1.dat\")\n",
    "        temp_df = pd.read_csv(file_path, sep='\\s+', index_col=\"ID\")\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = RGI_region[3:5] + '.' + temp_df.index.astype(str)\n",
    "        areas[RGI_region] = temp_df.mul(1e6)  # Convert km^2 to m^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "341a0c96-8d07-4c1f-a8dd-59d241ba1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(1980, 1, 1)\n",
    "end_date = datetime.date(2020, 1, 1)\n",
    "new_indices = pd.date_range(start_date, end_date, freq='A').strftime('%Y-%m').tolist()\n",
    "\n",
    "# Apply new index and datetime conversion\n",
    "for RGI_region, region_name in regions.items():\n",
    "    regional_solid_prcp[RGI_region].columns = new_indices\n",
    "    regional_solid_prcp[RGI_region].columns = pd.to_datetime(new_indices)\n",
    "    regional_liq_prcp[RGI_region].columns = new_indices\n",
    "    regional_liq_prcp[RGI_region].columns = pd.to_datetime(new_indices)\n",
    "    areas[RGI_region].columns = new_indices\n",
    "    areas[RGI_region].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40ef5ead-7c3e-4ee4-8202-46675d5161ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use initial area to compute total volume of precip components\n",
    "\n",
    "total_liq_prcp = {} \n",
    "total_solid_prcp = {} \n",
    "\n",
    "for RGI_region, region_name in regions.items():\n",
    "    total_liq_prcp[RGI_region] = pd.concat([areas[RGI_region] * regional_liq_prcp[RGI_region]], axis=1)\n",
    "    total_solid_prcp[RGI_region] = pd.concat([areas[RGI_region] * regional_solid_prcp[RGI_region]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2cd4b51-ec13-422f-a875-c0a60b119eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prcp = {} \n",
    "\n",
    "for RGI_region, region_name in regions.items():\n",
    "    total_prcp[RGI_region] = total_solid_prcp[RGI_region] + total_liq_prcp[RGI_region]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f910a-0962-4874-a9f5-4d27b106cac5",
   "metadata": {},
   "source": [
    "#### Now aggregating by basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67a5127d-ff92-4203-b5cd-06b0a0431297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a74b028-0987-4f30-8d23-842ddddfdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_basin(basin_RGI_list, precip_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [str(x)[-8:] for x in basin_RGI_list]\n",
    "    \n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in precip_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = precip_data.loc[new_basin_list].copy()\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    summed_basin_precip = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return summed_basin_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0ad982b-1523-4dad-9eb4-2b2bb3a254f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_gls = {}\n",
    "for basin, ID in basins.items():\n",
    "    basin_gls[basin] = select_glaciers_json(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12fb1a5a-c767-4af0-a132-b7c928025f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_precip_totals = {}\n",
    "for basin, ID in basins.items():\n",
    "    basin_precip_totals[basin] = {}\n",
    "    for RGI_region, region_name in regions.items():\n",
    "        basin_precip_totals[basin][RGI_region] = sum_basin(basin_gls[basin], total_prcp[RGI_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c4404c5-6821-4cb6-b692-56679d042a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining basin sums for same basins that cross RGI region boundaries\n",
    "total_prcp = {}\n",
    "for b, basin in enumerate(basins):\n",
    "    total_prcp[basin] = 0\n",
    "    for r, region in enumerate(regions):\n",
    "        total_prcp[basin] += basin_precip_totals[basin].get(region, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8383f996-3306-4b5c-9dd4-7ab5cd19d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prcp = {}\n",
    "for b, basin in enumerate(basins):\n",
    "    avg_prcp[basin] = total_prcp[basin][-20:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee44c6d1-997b-4c29-b8b8-a71ebf9282dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_list = []\n",
    "\n",
    "# Iterate over basins to extract the mean total precipitation\n",
    "for basin in basins:\n",
    "    # Extract the mean total precipitation for the current basin\n",
    "    mean_total_precipitation = avg_prcp[basin]\n",
    "    precipitation_list.append(mean_total_precipitation)\n",
    "\n",
    "# Create a pandas Series with basin names as index and mean total precipitation as values\n",
    "precipitation_series = pd.Series(precipitation_list, index=basins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd934daa-fe34-4fa2-ada1-2230078191c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RHINE          1.080036e+09\n",
       "RHONE          2.562059e+09\n",
       "PO             8.501258e+08\n",
       "DANUBE         1.006785e+09\n",
       "TITICACA       5.161067e+08\n",
       "                   ...     \n",
       "GANGES         1.856076e+10\n",
       "BRAHMAPUTRA    2.583841e+10\n",
       "OB             1.423324e+09\n",
       "INDIGIRKA      2.294758e+08\n",
       "YANGTZE        2.646151e+09\n",
       "Length: 75, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precipitation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f22e0bf-e0bc-4683-ae92-e1e572affaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the CSV files\n",
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Precipitation Analysis/'\n",
    "\n",
    "fname = f\"precip_avgs_Glo.csv\"\n",
    "\n",
    "# Define the full path of the output file\n",
    "output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "precipitation_series.to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f3758-1e87-43e8-9f46-33ad2f67242b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
