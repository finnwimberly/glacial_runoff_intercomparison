{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c233c2-7855-4acd-a7db-3a36564a2bd4",
   "metadata": {},
   "source": [
    "## Aggregating Net Basin Runoff-GLoGem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3c1f1-49c9-4383-a6f8-f23ae0a2ce90",
   "metadata": {},
   "source": [
    "#### Loading in all Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75655ad-869e-43bc-8ead-4f02e73f1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import date\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "## Generic the filepath to the main data folder\n",
    "fpath0 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/GloGEM Outputs/CentralEurope/files/'  \n",
    "fpath1 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/GloGEM Outputs/Runoff-intercomparison/GloGEM/Volume' \n",
    "\n",
    "#All of the climate models used\n",
    "modelnames = ['BCC-CSM2-MR','CAMS-CSM1-0','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0']\n",
    "\n",
    "SSPpaths = ['ssp126','ssp245','ssp370','ssp585']   #Specifiying the SSP\n",
    "SSPs = ['ssp119','ssp126','ssp245','ssp370','ssp585'] #Use a different path as we have all 5 ssps for volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c3edc",
   "metadata": {},
   "source": [
    "### Processing Runoff Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_discharges = [[] for _ in SSPpaths]\n",
    "\n",
    "for s, SSPpath in enumerate(SSPpaths):\n",
    "    model_discharges = []\n",
    "    for modelname in modelnames:\n",
    "        temp_df = pd.read_csv(fpath0 + modelname + '/' + SSPpaths[s]  + '/' + 'centraleurope_Discharge_r1.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        model_discharges.append(temp_df)\n",
    "    all_discharges[s] = model_discharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_areas = [[] for _ in SSPpaths]\n",
    "\n",
    "for s, SSPpath in enumerate(SSPpaths):\n",
    "    model_areas = []\n",
    "    for modelname in modelnames:\n",
    "        temp_df = pd.read_csv(fpath0 + modelname  + '/' + SSPpaths[s]  + '/' + 'centraleurope_Area_r1.dat', sep='\\s+', index_col=\"ID\")\n",
    "        model_areas.append(temp_df)\n",
    "    all_areas[s] = model_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(1979, 10, 1)\n",
    "end_date = datetime.date(2100, 9, 1)\n",
    "new_indices = pd.date_range(start_date, end_date, freq='MS').strftime('%Y-%m').tolist()\n",
    "\n",
    "# Apply new index and datetime conversion\n",
    "for s, SSPpath_discharges in enumerate(all_discharges):\n",
    "    for m, discharge_df in enumerate(SSPpath_discharges):\n",
    "        all_discharges[s][m].columns = new_indices\n",
    "        all_discharges[s][m].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e80a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expanding area dataset to match year-month dimension\n",
    "for s in range(len(SSPpaths)):\n",
    "    for i in range(len(all_areas[s])):\n",
    "        all_areas[s][i] = all_areas[s][i][all_areas[s][i].columns.repeat(12)]\n",
    "        \n",
    "for s, areas in enumerate(all_areas):\n",
    "    for i, area in enumerate(areas):\n",
    "        all_areas[s][i].columns = new_indices\n",
    "        all_areas[s][i].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb63a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use our initial area to compute runoff so we fill entire dfs with element 0\n",
    "# We only use one SSP because the initial areas are all the same -- we save time without looping through all\n",
    "# We also convert km^2 to m^2\n",
    "\n",
    "initial_areas = [pd.DataFrame(df.iloc[:, 0].values.repeat(df.shape[1]).reshape(df.shape), index=df.index, columns=df.columns).mul(1e6) for df in all_areas[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e5762-4f0f-4c97-a9b5-10de242627ac",
   "metadata": {},
   "source": [
    "#### Having properly indexed our dataframes, we define the runoff of each glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a02e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff = {s: {m: None for m in modelnames} for s in SSPpaths} # create nested dictionary indexed by model name and ssp\n",
    "n=0\n",
    "for s in SSPpaths:\n",
    "    i=0\n",
    "    for m in modelnames:\n",
    "        runoff[s][m] = pd.concat([initial_areas[i] * all_discharges[n][i]], axis=1)\n",
    "        i+=1\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "annualrunoff = {s: {m: None for m in modelnames} for s in SSPpaths}\n",
    "for s, m in itertools.product(SSPpaths, modelnames):\n",
    "    annualrunoff[s][m] = runoff[s][m].transpose().resample('A').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5122b",
   "metadata": {},
   "source": [
    "### Processing Volume Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cce13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data for each SSP\n",
    "all_volumes = [pd.read_csv(os.path.join(fpath1, SSP, 'RGIreg11_Volume_individual.dat'), sep='\\s+', header=0, skiprows=1, index_col=0) for SSP in SSPs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our new index\n",
    "new_indices = []\n",
    "for i in range(len(all_volumes[0])):\n",
    "    new_indices.append(int(((all_volumes[0].index[i]-11)*10**5))) #Here we just treat the RGI ID as a float to extract the last digits\n",
    "for i in range(len(SSPs)):\n",
    "    for n in range(len(all_volumes[i])):\n",
    "        all_volumes[i].index = new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed07cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to convert ice volume to water volume and km^3 to m^3\n",
    "water_volumes = [(df * 0.9 * 10**6) for df in all_volumes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69836ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a new data set showing the change in volume\n",
    "glacial_change = []\n",
    "for i, vol_df in enumerate(water_volumes):\n",
    "    diff_df = vol_df.diff(axis=1)\n",
    "    diff_df.drop(diff_df.columns[0], axis=1, inplace=True)  # deleting first column so we can insert diff values\n",
    "    glacial_change.append(diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f159d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary indexed by ssp\n",
    "glacial_runoff = {s: -df for s, df in zip(SSPs, glacial_change)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127b63d",
   "metadata": {},
   "source": [
    "### Aggregating by basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea454b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e289816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_basin(basin_RGI_list, runoff_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [int(str(x)[-4:]) for x in basin_RGI_list]\n",
    "    runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in runoff_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = runoff_data.loc[new_basin_list].copy()\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return summed_basin_runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0853cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the aggregated basin data\n",
    "alpine_basins = {'RHINE':'6242', 'RHONE':'6243','PO':'6241', 'DANUBE':'6202'} ## GRDC Major River Basin identifiers for the 3 alpine basins we can study\n",
    "\n",
    "Rhone_runoff = {s: {m: None for m in modelnames} for s in SSPpaths} # create nested dictionary indexed by model name and ssp\n",
    "Rhine_runoff = {s: {m: None for m in modelnames} for s in SSPpaths} \n",
    "Po_runoff = {s: {m: None for m in modelnames} for s in SSPpaths}\n",
    "Danube_runoff = {s: {m: None for m in modelnames} for s in SSPpaths} \n",
    "\n",
    "for s in SSPpaths:\n",
    "    for m in modelnames:\n",
    "        Rhine_runoff[s][m] = sum_basin(select_glaciers_json(alpine_basins['RHINE']), annualrunoff[s][m])*1e-9    #m^3 to km^3\n",
    "        Rhone_runoff[s][m] = sum_basin(select_glaciers_json(alpine_basins['RHONE']), annualrunoff[s][m])*1e-9\n",
    "        Po_runoff[s][m] = sum_basin(select_glaciers_json(alpine_basins['PO']), annualrunoff[s][m])*1e-9\n",
    "        Danube_runoff[s][m] = sum_basin(select_glaciers_json(alpine_basins['DANUBE']), annualrunoff[s][m])*1e-9\n",
    "\n",
    "        \n",
    "rollingmean_Rhone = {s: {m: Rhone_runoff[s][m].rolling(30).mean() for m in modelnames} for s in SSPpaths}\n",
    "rollingmean_Rhine = {s: {m: Rhine_runoff[s][m].rolling(30).mean() for m in modelnames} for s in SSPpaths}\n",
    "rollingmean_Po = {s: {m: Po_runoff[s][m].rolling(30).mean() for m in modelnames} for s in SSPpaths}\n",
    "rollingmean_Danube = {s: {m: Danube_runoff[s][m].rolling(30).mean() for m in modelnames} for s in SSPpaths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our color scheme\n",
    "color_map = plt.colormaps['magma']\n",
    "colors = {model: color_map((i)/len(modelnames)) for i, model in enumerate(modelnames)}\n",
    "\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "yrs_runoff = np.linspace(1980,2100, num=122)\n",
    "\n",
    "fig, axs = plt.subplots(len(scenarios), 4, figsize=(12, 10), sharex=True)\n",
    "\n",
    "for i, s in enumerate(scenarios):\n",
    "    axs[i, 0].set(xlim=(2000, 2100), ylim=(.95, 1.55))              #Plotting Rhine\n",
    "    axs[i, 0].text(2070, 1.49, s)\n",
    "    for m in modelnames:\n",
    "        axs[i, 0].plot(yrs_runoff, rollingmean_Rhine[s][m], label=m, c=colors[m])\n",
    "for i, s in enumerate(scenarios):\n",
    "    axs[i, 1].set(xlim=(2000, 2100), ylim=(2.3, 3.9)) \n",
    "    axs[i, 1].text(2070, 3.75, s)                                 #Plotting Rhone\n",
    "    for m in modelnames:\n",
    "        axs[i, 1].plot(yrs_runoff, rollingmean_Rhone[s][m], label=m, c=colors[m])\n",
    "    \n",
    "for i, s in enumerate(scenarios):\n",
    "    axs[i, 2].set(xlim=(2000, 2100), ylim=(0.65, 1.2))               #Plotting Po\n",
    "    axs[i, 2].text(2070, 1.15, s) \n",
    "    for m in modelnames:\n",
    "        axs[i, 2].plot(yrs_runoff, rollingmean_Po[s][m], label=m, c=colors[m])\n",
    "\n",
    "for i, s in enumerate(scenarios):\n",
    "    axs[i, 3].set(xlim=(2000, 2100), ylim=(.78, 1.45))              #Plotting Danube\n",
    "    axs[i, 3].text(2070, 1.39, s)\n",
    "    for m in modelnames:\n",
    "        axs[i, 3].plot(yrs_runoff, rollingmean_Danube[s][m], label=m, c=colors[m])\n",
    "\n",
    "#Setting x labels\n",
    "if i == 3:\n",
    "    axs[i, 0].set_xlabel('Year')\n",
    "    axs[i, 1].set_xlabel('Year')\n",
    "    axs[i, 2].set_xlabel('Year')\n",
    "    axs[i, 3].set_xlabel('Year')\n",
    "#Setting y labels\n",
    "for i in range(4):\n",
    "    axs[i, 0].set_ylabel(r'Rolling Mean Runoff $[km^3]$')\n",
    "        \n",
    "plt.subplots_adjust(top=0.85, wspace=0.3, hspace=0.2)\n",
    "plt.suptitle('Runoff of Major River Basins in Central Europe, Projected by GloGEM')\n",
    "plt.title('Rhine River Basin                   Rhone River Basin                      Po River Basin                   Danube River Basin', x=-1.43, y=4.72)\n",
    "axs[i, 0].legend(bbox_to_anchor=(4.96, 5.27), ncol=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e36cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
