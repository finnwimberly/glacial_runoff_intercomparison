{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a49e0e6-a595-4c69-9986-92487b128174",
   "metadata": {},
   "source": [
    "## Comparison of GloGEM, PyGEM, and OGGM RGI 17 Volume Outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a803-ff08-4d30-a540-bbd5df8e6233",
   "metadata": {},
   "source": [
    "This notebook imports and processes GloGEM, PyGEM, and OGGM RGI 17 volume outpts. Summing glacial volume change by basin, we produce a plot that compares the three models' projected volume values for each basin by SSP. \n",
    "\n",
    "Last Updated: 28 June 2023 | FFW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3bfac-2952-49bf-8005-2023fb4f8681",
   "metadata": {},
   "source": [
    "## Loading in data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa2cdb-7561-40d3-add1-375fd9a533e2",
   "metadata": {},
   "source": [
    "### GloGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcf90b-fb5a-4952-8f6b-37c81b313e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import date\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "## Generic the filepath to the main data folder\n",
    "fpath0 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/'\n",
    "fpath1 = 'Runoff-intercomparison/GloGEM-output/Volume_GloGEM-20230626/SouthernAndes/' \n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_glo = ['BCC-CSM2-MR','CAMS-CSM1-0','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #Use a different path as we have all 5 ssps for volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378cbeb-8bf7-41a6-9336-78fe865ebdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        temp_df = pd.read_csv(fpath0 + fpath1 + model + '/' + SSP  + '/' + 'Volume_SouthernAndes.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        volumes[SSP][model] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fea77-bc8d-4eb0-b44f-0407255d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(1980, 1, 1)\n",
    "end_date = datetime.date(2101, 12, 1)\n",
    "new_indices = pd.date_range(start_date, end_date, freq='A').strftime('%Y-%m').tolist()\n",
    "\n",
    "# Apply new index and datetime conversion\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        volumes[SSP][model].columns = new_indices\n",
    "        volumes[SSP][model].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824c33e-f2d0-42b1-b37d-1948dfe8bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdcf98-6ddb-47bb-a6e0-64c48a5e4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_basin(basin_RGI_list, runoff_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [int(str(x)[-5:]) for x in basin_RGI_list]\n",
    "    #runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers within a basin that are not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in runoff_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = runoff_data.loc[new_basin_list].copy()\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return summed_basin_runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350d14e-2a93-49a5-a6b5-064986afe987",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpine_basins = {'YELCHO':'3429', 'VALDIVIA':'3428', 'SERRANO':'3426', 'RAPEL':'3423', 'PUELO':'3422', \n",
    "                'PASCUA':'3420', 'PALENA':'3419', 'HUASCO':'3412', 'COPIAPO':'3409', 'CISNES':'3408', \n",
    "                'BIOBIO':'3405', 'BAKER':'3404', 'AZOPARDO':'3403', 'AISEN':'3401', 'SANTA CRUZ':'3244', \n",
    "                'NEGRO':'3232', 'COLORADO':'3212', 'CHICO':'3209'} \n",
    "\n",
    "basins = ['YELCHO', 'VALDIVIA', 'SERRANO','RAPEL','PUELO', 'PASCUA', 'PALENA', 'HUASCO', 'COPIAPO', \n",
    "          'CISNES', 'BIOBIO', 'BAKER', 'AZOPARDO', 'AISEN', 'SANTA CRUZ', 'NEGRO', 'COLORADO', 'CHICO']\n",
    "\n",
    "basin_sums_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        basin_sums_glo[SSP][basin] = {}\n",
    "        for m, model in enumerate(modelnames_glo):\n",
    "            basin_sums_glo[SSP][basin][model] = sum_basin(select_glaciers_json(alpine_basins[basin]), volumes[SSP][model]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c158857-c086-4172-a155-0cf7a54e0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate multi GCM means and Quartiles we convert to df then calculate across first axis (GCMs)\n",
    "GCM_mean_glo = {}\n",
    "GCM_q1_glo = {}\n",
    "GCM_q3_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glo[SSP] = {}\n",
    "    GCM_q1_glo[SSP] = {}\n",
    "    GCM_q3_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056ce7a-25f5-4ab1-9c1d-8ee835c32cfb",
   "metadata": {},
   "source": [
    "### PyGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0ec41-d200-4ad3-914c-9e52d4a8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_py = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #List of all SSPs in PyGEM\n",
    "\n",
    "alpine_basins = {'YELCHO':'3429', 'VALDIVIA':'3428', 'SERRANO':'3426', 'RAPEL':'3423', 'PUELO':'3422', \n",
    "                'PASCUA':'3420', 'PALENA':'3419', 'HUASCO':'3412', 'COPIAPO':'3409', 'CISNES':'3408', \n",
    "                'BIOBIO':'3405', 'BAKER':'3404', 'AZOPARDO':'3403', 'AISEN':'3401', 'SANTA CRUZ':'3244', \n",
    "                'NEGRO':'3232', 'COLORADO':'3212', 'CHICO':'3209'} \n",
    "\n",
    "basins = ['YELCHO', 'VALDIVIA', 'SERRANO','RAPEL','PUELO', 'PASCUA', 'PALENA', 'HUASCO', 'COPIAPO', \n",
    "          'CISNES', 'BIOBIO', 'BAKER', 'AZOPARDO', 'AISEN', 'SANTA CRUZ', 'NEGRO', 'COLORADO', 'CHICO']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathPy = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/Runoff-intercomparison/PyGEM/17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2967b5-7aa8-4093-9c03-3810c90de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_gls = {}\n",
    "for basin, code in alpine_basins.items():\n",
    "    basin_gls[basin] = select_glaciers_json(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f30f4-4d2d-4731-9391-658d6193889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all runoff data\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "volume_ds = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '/mass_annual/R17_mass_annual_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_mass_annual.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        volume_ds[SSP] = combined_ds * 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a7250-2b42-418d-837c-572cd2e61d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting into basins\n",
    "basin_volumes = {}\n",
    "for basin, glacier_list in basin_gls.items():\n",
    "    ## loop over them all, drop the irrelevant IDs, and concatenate the result\n",
    "    basin_volumes[basin] = {}\n",
    "    for s, SSP in enumerate(SSPs):\n",
    "        ds_list = []\n",
    "        try:\n",
    "            ds_filtered = volume_ds[SSP].where(volume_ds[SSP].RGIId.isin(glacier_list), drop=True)\n",
    "            #print(ds_filtered)\n",
    "            ds_list.append(ds_filtered)\n",
    "        except ValueError: ## happens if there are no glaciers from this batch in the selected region\n",
    "            continue\n",
    "        basin_volumes[basin][SSP] = xr.concat(ds_list, dim='glacier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7996f-47c3-4db5-ae5f-ca8af7059282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping indexing (to match other models), summing basins, and converting kg to km^3\n",
    "basin_sums_py = {}\n",
    "for s, SSP in enumerate(SSPs):        \n",
    "    basin_sums_py[SSP] = {}\n",
    "    for basin, glacier_list in basin_gls.items():\n",
    "        basin_sums_py[SSP][basin] = basin_volumes[basin][SSP].sum(dim='glacier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63462f93-6ceb-41da-84eb-7c57d9827bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_sums_py['ssp126']['BIOBIO'][::,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5020d-7507-4ca5-b7c4-7c11a6983292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles\n",
    "GCM_mean_py = {}\n",
    "GCM_q1_py = {}\n",
    "GCM_q3_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_py[SSP] = {}\n",
    "    GCM_q1_py[SSP] = {}\n",
    "    GCM_q3_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_py[SSP][basin] = basin_sums_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.75, dim = 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05646bfc-a8bf-463f-a2c3-689a26ebeeb5",
   "metadata": {},
   "source": [
    "### OGGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a65312-93f8-4642-9be9-5c0a879f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the climate models used\n",
    "modelnames_OG = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5','EC-Earth3', \n",
    "                'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8','INM-CM5-0', \n",
    "                 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM', 'TaiESM1']\n",
    "\n",
    "alpine_basins = {'YELCHO':'3429', 'VALDIVIA':'3428', 'SERRANO':'3426', 'RAPEL':'3423', 'PUELO':'3422', \n",
    "                'PASCUA':'3420', 'PALENA':'3419', 'HUASCO':'3412', 'COPIAPO':'3409', 'CISNES':'3408', \n",
    "                'BIOBIO':'3405', 'BAKER':'3404', 'AZOPARDO':'3403', 'AISEN':'3401', 'SANTA CRUZ':'3244', \n",
    "                'NEGRO':'3232', 'COLORADO':'3212', 'CHICO':'3209'} \n",
    "\n",
    "# CMCC-CM2-SR5 & TaiESM1 only hold values for ssp585––this is model list without those GCMS\n",
    "modelnames_OG_trimmed = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', \n",
    "                         'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8',\n",
    "                           'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathOG1 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/'\n",
    "fpathOG2 = 'Lizz Research Stuff/Runoff-intercomparison/OGGM/lschuster/runs_2023.3/output/basins/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3292ca-f2a4-41ff-8926-ed580123969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all runoff data, OGGM is grouped by basin\n",
    "rf_ds = {}\n",
    "for basin, ID in alpine_basins.items():\n",
    "    fpath_basin = 'gcm_from_2000_bc_2000_2019/{}/'.format(ID)\n",
    "    #print(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc')\n",
    "    with xr.open_mfdataset(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc') as ds:\n",
    "        ds = ds.volume.load()\n",
    "    rf_ds[basin] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb146f21-50db-434e-9923-92af7243fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summing individual glacier runoff into basin totals and converting m^3 to km^3\n",
    "basin_volume_OG = {}\n",
    "for basin, ID in alpine_basins.items():\n",
    "    basin_volume_OG[basin] = rf_ds[basin].sum(dim = 'rgi_id') * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334a8c3-4629-46da-ae1f-ce6164dcd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dict of GloPy format\n",
    "basins = ['YELCHO', 'VALDIVIA', 'SERRANO','RAPEL','PUELO', 'PASCUA', 'PALENA', 'HUASCO', 'COPIAPO', \n",
    "          'CISNES', 'BIOBIO', 'BAKER', 'AZOPARDO', 'AISEN', 'SANTA CRUZ', 'NEGRO', 'COLORADO', 'CHICO']\n",
    "basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        basin_sums_OG[SSP][basin] = basin_volume_OG[basin].sel(scenario = SSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffed482-8ace-44ec-b183-576daedb3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing these GCMs for ALL SSPs--doing even 585 as these two are not included...\n",
    "#... in Glo or Py so not only makeds OOGM easier but maintains GCM consistency in analysis\n",
    "trimmed_basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    trimmed_basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        trimmed_basin_sums_OG[SSP][basin] = xr.concat([basin_sums_OG[SSP][basin][0:4], basin_sums_OG[SSP][basin][5:-1]], dim='gcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549a69c-5659-4c7c-9dd9-6b28880e629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_basin_sums_OG['ssp126']['BIOBIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a31c0-7508-462a-bf9f-8ae77c55e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles for OGGM\n",
    "GCM_mean_OG = {}\n",
    "GCM_q1_OG = {}\n",
    "GCM_q3_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    which_ssp = SSPs[s]\n",
    "    GCM_mean_OG[which_ssp] = {}\n",
    "    GCM_q1_OG[which_ssp] = {}\n",
    "    GCM_q3_OG[which_ssp] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.75, dim = 'gcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aeb11-cb89-4d48-93ae-c55392865f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot setup\n",
    "from cycler import cycler\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['YELCHO', 'VALDIVIA', 'SERRANO','RAPEL','PUELO', 'PASCUA', 'PALENA', 'HUASCO', 'COPIAPO', \n",
    "          'CISNES', 'BIOBIO', 'BAKER', 'AZOPARDO', 'AISEN', 'SANTA CRUZ', 'NEGRO', 'COLORADO', 'CHICO']\n",
    "\n",
    "basinstext = ['Yelcho', 'Valdivia', 'Serrano','Rapel','Puelo', 'Pascua', 'Palena', 'Huasco', 'Copiapo', \n",
    "          'Cisnes', 'Biobio', 'Baker', 'Azopardo', 'Aisen', 'Santa Cruz', 'Negro', 'Colorado', 'Chico']\n",
    "\n",
    "yrs_glo = np.arange(1980,2101)\n",
    "yrs_glo_dt = pd.to_datetime([str(y)for y in yrs_glo])\n",
    "\n",
    "colors_glo =  plt.colormaps['Greens']\n",
    "line_colors_glo = colors_glo(np.linspace(0.2, 0.6, num = 12))\n",
    "glo_cycler = cycler(color = line_colors_glo)\n",
    "\n",
    "colors_py =  plt.colormaps['Purples']\n",
    "line_colors_py = colors_py(np.linspace(0.2, 0.6,num = 12))\n",
    "py_cycler = cycler(color = line_colors_py)\n",
    "\n",
    "colors_OG =  plt.colormaps['Blues']\n",
    "line_colors_OG = colors_OG(np.linspace(0.2, 0.6,num = 12))\n",
    "OG_cycler = cycler(color = line_colors_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c4d8f-9b56-4625-8aa8-99786c9dca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_sums_py['ssp126']['BIOBIO'].sel(model = 1)[0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc47ae1-95f5-40ca-a95b-b6e507c191a0",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7cae1c-44e2-4bc3-aad6-f33ef553c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all basins contain the same glaciers:\n",
    "num_glac_in_basin_OG = {}\n",
    "num_glac_in_basin_glo = {}\n",
    "num_glac_in_basin_py = {}\n",
    "for b, basin in enumerate(basins):\n",
    "    num_glac_in_basin_OG[basin] = rf_ds[basin].rgi_id.size    #OGGM\n",
    "    num_glac_in_basin_glo[basin] = len(select_glaciers_json(alpine_basins[basin]))   #GloGEM\n",
    "    num_glac_in_basin_py[basin] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        num_glac_in_basin_py[basin][SSP] = basin_volumes[basin][SSP].RGIId.size     #PyGEM  \n",
    "\n",
    "#PyGEM: palena 650, santa cruz 461\n",
    "#OGGM: palena 651, santa cruz 462\n",
    "#GloGEM: palena 651, santa cruz 462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccefee4-13de-4d4d-84ea-6dcf7671fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 28), sharex=True)\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "\n",
    "        #OG won't plot with built-in ds.plot()\n",
    "        #Trim last value as it goes to zero\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], trimmed_basin_sums_OG[which_ssp][basin][:,0:-1].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'dodgerblue', alpha = 0.5)\n",
    "\n",
    "        #Trim first value as it is incomplete hydrological year\n",
    "        for m in modelnames_glo:\n",
    "            axs[b, s].plot(yrs_glo_dt, basin_sums_glo[which_ssp][basin][m], color=axs[b, s].set_prop_cycle(glo_cycler), alpha = 0.95)\n",
    "        axs[b,s].plot(yrs_glo_dt, GCM_mean_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt, GCM_q1_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt, GCM_q3_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt, GCM_q1_glo[which_ssp][basin], GCM_q3_glo[which_ssp][basin], color = 'green')\n",
    "        axs[b, s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[b,s].plot(yrs_glo_dt[20::], basin_sums_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_mean_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.5)\n",
    "\n",
    "        #Make mean more clear for RHONE, which overlaps significantly w Glo\n",
    "        #axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.9)\n",
    "\n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[b, sub_s].set_xlabel('Year')\n",
    "                axs[b, sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[b, s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            for sub_b in range(len(basins)):\n",
    "                axs[sub_b,s].set_ylabel(basinstext[sub_b]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[b, s].set_ylabel(None)\n",
    "            axs[b, s].set_yticklabels('')\n",
    "\n",
    "for b in range(len(basins)):         \n",
    "    row_min = np.inf\n",
    "    row_max = -np.inf                 \n",
    "    for s in range(len(SSPs)):\n",
    "        data_min = np.min(axs[b, s].get_ybound()[0])\n",
    "        data_max = np.max(axs[b, s].get_ybound()[1])\n",
    "        if data_min < row_min:\n",
    "            row_min = data_min\n",
    "        if data_max > row_max:\n",
    "            row_max = data_max\n",
    "            #row_max[basin] = data_max\n",
    "    for s in range(len(SSPs)):\n",
    "        axs[b, s].set_ylim(row_min, row_max)\n",
    "\n",
    "#Adding in text of # of glaciers in each basin\n",
    "row_max_values = []\n",
    "for b, basin in enumerate(basins):\n",
    "    row_max = max(axs[b, s].get_ylim()[1] for s in range(len(SSPs)))\n",
    "    row_max_values.append(row_max)\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    for b, basin in enumerate(basins):\n",
    "        y_coord = row_max_values[b] * 0.85\n",
    "        axs[b,s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_glo[basin]}\", fontsize=8)\n",
    "        #axs[b,s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_OG[basin]}\", fontsize=8)\n",
    "        #axs[b,s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_py[basin]['ssp126']}\", fontsize=8)\n",
    "       \n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0,0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, 1.71), ncol=3)\n",
    "\n",
    "plt.suptitle('Glacial Volume Change of Major Southern Andes River Basins', x=0.52, y=0.915)\n",
    "plt.title('SSP 126                            SSP 245                           SSP 370                            SSP 585', x=-1.3, y=21.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cff0b5-1807-4519-83af-746678a31d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "change_results = []  # List to store the change results\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    for b, basin in enumerate(basins):\n",
    "        max_change = 0.0  # Initialize the maximum change\n",
    "        max_change_year = None  # Variable to store the year of maximum change\n",
    "        max_change_model = None  # Variable to store the model predicting the maximum change\n",
    "\n",
    "        for m, model in enumerate(modelnames_glo):\n",
    "            values = basin_sums_glo[SSP][basin][model]\n",
    "            changes = np.diff(values)  # Calculate the volume change between consecutive years\n",
    "            abs_changes = np.abs(changes)  # Take the absolute value of the changes\n",
    "            max_value = np.max(values)  # Maximum value in the values array\n",
    "            relative_changes = changes / max_value  # Calculate relative changes\n",
    "\n",
    "            max_index = np.argmax(abs_changes)  # Find the index of the maximum absolute change\n",
    "            max_change_value = relative_changes[max_index]  # Get the maximum relative change value\n",
    "\n",
    "            if np.abs(max_change_value) > np.abs(max_change):\n",
    "                max_change = max_change_value\n",
    "                max_change_year = max_index + 1  # Add 1 to get the year index\n",
    "                max_change_model = model\n",
    "\n",
    "        # Append the results for the current SSP and basin to the list\n",
    "        change_results.append((SSP, basin, max_change, max_change_year, max_change_model))\n",
    "\n",
    "# Sort the change results in descending order based on the absolute relative change size\n",
    "change_results.sort(key=lambda x: np.abs(x[2]), reverse=True)\n",
    "\n",
    "# Print and save the sorted results\n",
    "csv_file = \"percent_change_results.csv\"\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"SSP\", \"Basin\", \"Change\", \"Year\", \"Model\"])  # Write header row\n",
    "\n",
    "    for result in change_results:\n",
    "        SSP, basin, max_change, max_change_year, max_change_model = result\n",
    "        #print(f\"SSP: {SSP}, Basin: {basin}, Percent Change: {max_change:.2f}, Year: {max_change_year}, Model: {max_change_model}\")\n",
    "\n",
    "        #Commenting out to avoid continually overwriting file\n",
    "        #writer.writerow([SSP, basin, max_change, max_change_year +1980, max_change_model])\n",
    "\n",
    "print(f\"Results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467ded0-d60a-401e-9a31-bcf4f0404b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_results = []  # List to store the change results\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    for b, basin in enumerate(basins):\n",
    "        max_change = 0.0  # Initialize the maximum change\n",
    "        max_change_year = 0  # Variable to store the year of maximum change\n",
    "        max_change_model = None  # Variable to store the model predicting the maximum change\n",
    "\n",
    "        for m, model in enumerate(modelnames_glo):\n",
    "            values = basin_sums_glo[SSP][basin][model]\n",
    "            changes = np.diff(values)  # Calculate the volume change between consecutive years\n",
    "            abs_changes = np.abs(changes)  # Take the absolute value of the changes\n",
    "            max_index = np.argmax(abs_changes)  # Find the index of the maximum absolute change\n",
    "            max_change_value = changes[max_index]  # Get the maximum change value\n",
    "\n",
    "            if np.abs(max_change_value) > np.abs(max_change):\n",
    "                max_change = max_change_value\n",
    "                max_change_year = max_index + 1  # Add 1 to get the year index\n",
    "                max_change_model = model\n",
    "\n",
    "        # Append the results for the current SSP and basin to the list\n",
    "        change_results.append((SSP, basin, max_change, max_change_year + 1980, max_change_model))\n",
    "\n",
    "# Sort the change results in descending order based on the absolute change size\n",
    "change_results.sort(key=lambda x: np.abs(x[2]), reverse=True)\n",
    "\n",
    "# Print and save the sorted results\n",
    "csv_file = \"change_results.csv\"\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"SSP\", \"Basin\", \"Change km^3\", \"Year\", \"Model\"])  # Write header row\n",
    "\n",
    "    for result in change_results:\n",
    "        SSP, basin, max_change, max_change_year, max_change_model = result\n",
    "        #print(f\"SSP: {SSP}, Basin: {basin}, Change: {max_change:.2f} km^3, Year: {max_change_year}, Model: {max_change_model}\")\n",
    "\n",
    "        #Commenting out to avoid continually overwriting file\n",
    "        #writer.writerow([SSP, basin, max_change, max_change_year, max_change_model])\n",
    "\n",
    "print(f\"Results saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b5f7a-2688-469e-a77e-240d6e94b31d",
   "metadata": {},
   "source": [
    "### Runoff from volume change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2442b7-dd2b-46cc-8774-16cdc9efbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change in volume = runoff so:\n",
    "glacial_melt_glo = {}\n",
    "glacial_melt_OG = {}\n",
    "glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    glacial_melt_glo[SSP] = {}\n",
    "    glacial_melt_OG[SSP] = {}\n",
    "    glacial_melt_py[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        glacial_melt_glo[SSP][basin] = {}                                       #Converting neg volume change to pos runoff\n",
    "        for m, model in enumerate(modelnames_glo):                              #And ice volume to water volume\n",
    "            glacial_melt_glo[SSP][basin][model] = basin_sums_glo[SSP][basin][model][20::].diff()*-0.92\n",
    "        glacial_melt_OG[SSP][basin] = trimmed_basin_sums_OG[SSP][basin].diff(dim = 'time')*-0.92\n",
    "        glacial_melt_py[SSP][basin] = basin_sums_py[SSP][basin].diff(dim = 'year')*-0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c977b6-097d-4123-87fb-5f0637802c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing multi-GCM means and quartiles\n",
    "GCM_mean_glacial_melt_OG = {}\n",
    "GCM_q1_glacial_melt_OG = {}                         #OGGM\n",
    "GCM_q3_glacial_melt_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q1_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q3_glacial_melt_OG[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.75, dim = 'gcm')\n",
    "\n",
    "GCM_mean_glacial_melt_py = {}\n",
    "GCM_q1_glacial_melt_py = {}\n",
    "GCM_q3_glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_py[SSP] = {}\n",
    "    GCM_q1_glacial_melt_py[SSP] = {}                #PyGEM\n",
    "    GCM_q3_glacial_melt_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.75, dim = 'model')\n",
    "\n",
    "GCM_mean_glacial_melt_glo = {}\n",
    "GCM_q1_glacial_melt_glo = {}\n",
    "GCM_q3_glacial_melt_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q1_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q3_glacial_melt_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ba284-1577-476d-8437-db82851e0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 28), sharex=True)\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "        \n",
    "        for m in modelnames_glo:\n",
    "            axs[b, s].plot(yrs_glo_dt[20::], glacial_melt_glo[which_ssp][basin][m], color=axs[b, s].set_prop_cycle(glo_cycler), alpha = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_mean_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'green')\n",
    "        axs[b, s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], glacial_melt_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.2)\n",
    "\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], glacial_melt_OG[which_ssp][basin].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1],  GCM_q1_glacial_melt_OG[which_ssp][basin],  GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'dodgerblue', alpha = 0.25)\n",
    "        \n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[b, sub_s].set_xlabel('Year')\n",
    "                axs[b, sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[b, s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            for sub_b in range(len(basins)):\n",
    "                axs[sub_b,s].set_ylabel(basinstext[sub_b]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[b, s].set_ylabel(None)\n",
    "            axs[b, s].set_yticklabels('')\n",
    "\n",
    "for b in range(len(basins)):    #To look more closely at inter-quartile range \n",
    "    row_max = -np.inf\n",
    "    row_min = np.inf\n",
    "    data_list = []\n",
    "    for s in range(len(SSPs)):\n",
    "        data = np.concatenate([GCM_q3_glacial_melt_py[SSPs[s]][basins[b]][0:-1], GCM_q3_glacial_melt_glo[SSPs[s]][basins[b]], GCM_q3_glacial_melt_OG[SSPs[s]][basins[b]]])\n",
    "        data_list.extend(data[~np.isnan(data) & np.isfinite(data)])\n",
    "    if len(data_list) > 0:\n",
    "        row_max = np.max(data_list)\n",
    "        row_min = np.min(data_list)\n",
    "    if row_min >= 0:\n",
    "        bottom_limit = row_min / 1.5\n",
    "    else:\n",
    "        bottom_limit = row_min * 1.5\n",
    "    for s in range(len(SSPs)):\n",
    "        axs[b, s].set_ylim(bottom_limit, row_max)\n",
    "\n",
    "\n",
    "# for b in range(len(basins)):         #To looks at whole picture-limits determined by max/min between all GCMs\n",
    "#     row_min = np.inf\n",
    "#     row_max = -np.inf\n",
    "#     for s in range(len(SSPs)):\n",
    "#         data_min = np.min(axs[b, s].get_ybound()[0])\n",
    "#         data_max = np.max(axs[b, s].get_ybound()[1])\n",
    "#         if data_min < row_min:\n",
    "#             row_min = data_min\n",
    "#         if data_max > row_max:\n",
    "#             row_max = data_max\n",
    "#     for s in range(len(SSPs)):\n",
    "#         axs[b, s].set_ylim(row_min, row_max)\n",
    "\n",
    "\n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0,0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, 1.71), ncol=3)\n",
    "\n",
    "plt.suptitle('Glacier Volume Contribution to Runoff in Major Southern Andes River Basins', x=0.52, y=0.915)\n",
    "plt.title('SSP 126                            SSP 245                           SSP 370                            SSP 585', x=-1.3, y=21.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44494fa-8a73-492b-b238-4c52b600764e",
   "metadata": {},
   "source": [
    "### Total regional sum (within basins)\n",
    "Realized after the fact that this is only for glaciers located in one of our major river basins... not all glaciers in RGI-17. Calulated this complete sum below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06fe9-0b8e-47ad-91ab-27220a712936",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_basins_volume_py = {}\n",
    "total_basins_volume_OG = {}\n",
    "total_basins_volume_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    total_basins_volume_py[SSP] = xr.zeros_like(GCM_mean_py[SSP][basins[0]])\n",
    "    total_basins_volume_OG[SSP] = xr.zeros_like(GCM_mean_OG[SSP][basins[0]])\n",
    "    total_basins_volume_glo[SSP] = pd.Series(index=GCM_mean_glo[SSP][basins[0]].index, dtype=float)    \n",
    "    for b, basin in enumerate(basins):\n",
    "        total_basins_volume_py[SSP] += GCM_mean_py[SSP][basin]\n",
    "        total_basins_volume_OG[SSP] += GCM_mean_OG[SSP][basin]\n",
    "\n",
    "        temp_series = GCM_mean_glo[SSP][basin]    #A little different-working w pd.series\n",
    "        total_basins_volume_glo[SSP] = total_basins_volume_glo[SSP].add(temp_series, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d65cf1-8c85-4811-af50-770a0cbedb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_basins_volume_glo['ssp585'][20::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91b584-c455-4dec-9ceb-f8bdde3845e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(SSPs), figsize=(10, 2), sharex=True)\n",
    "\n",
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "yrs = np.arange(2000,2101)\n",
    "yrs_dt = pd.to_datetime([str(y)for y in yrs])\n",
    "\n",
    "for s, SSP in enumerate(scenarios):                #Plotting Volume Change\n",
    "    axs[s].plot(yrs_dt, total_basins_volume_OG[SSP], color = 'royalblue', linewidth = 0.9, label = 'OGGM')\n",
    "    axs[s].plot(yrs_dt, total_basins_volume_glo[SSP][20::], color = 'darkgreen', linewidth = 0.9, label = 'GloGEM')\n",
    "    axs[s].plot(yrs_dt, total_basins_volume_py[SSP][0:-1], color = 'purple', linewidth = 0.9, label = 'PyGEM')\n",
    "    \n",
    "    axs[s].set(title = '')\n",
    "\n",
    "    for s in range(4):  # Use a different variable name for the inner loop\n",
    "        axs[s].set_xlabel('Year')\n",
    "        axs[s].set_xticks([pd.to_datetime('2000'),pd.to_datetime('2050'), pd.to_datetime('2100')], [2000, 2050, 2100])\n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            axs[s].set_ylabel(r' Total Glacial Volume $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[s].set_ylabel('')\n",
    "            axs[s].set_yticklabels('')\n",
    "        \n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0].legend(handles=[green_patch, purple_patch, blue_patch],bbox_to_anchor=(3.15, 1.4), ncol=3)\n",
    "plt.suptitle('Composite, Intrabasin Glacial Volume for RGI Region 17', x=0.488, y=1.28)\n",
    "plt.title('SSP 126                          SSP 245                         SSP 370                          SSP 585', x=-1.3, y=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec892025-a4db-4ef7-8ba8-6e7dc22a3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_basins_volume_OG['ssp126'][0])\n",
    "print(total_basins_volume_py['ssp126'][0])\n",
    "print(total_basins_volume_glo['ssp126'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f8351-95cc-43fe-aa9e-b59be5972a75",
   "metadata": {},
   "source": [
    "### Total Regional Volume\n",
    "Will not be able to calculate this for OGGM as our loaded data was only for glaciers within GRDC major river basins, however, we have loaded ALL glaciers within RGI-17 for PyGEM and GloGEM. Here we will find to total glacial volume, for these two models, in RGI-17 to compare to Farinotti et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566dcf33-abb1-43b0-a32c-61720ee5d1d7",
   "metadata": {},
   "source": [
    "#### GloGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14537e-dbd3-469c-b07b-60534ef6fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_region_volume_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    total_region_volume_glo[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        total_region_volume_glo[SSP][model] = volumes[SSP][model].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ebb8b-f7bf-4ba9-bd8f-7baebb4d5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, model in enumerate(modelnames_glo):\n",
    "    print(total_region_volume_glo['ssp126'][model][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9aa7c-0553-4138-8e0d-0fd62aa93a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_region_volume_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    total_region_volume_py[SSP] = total_region_volume_glo[SSP] = volume_ds[SSP].sum(dim = 'glacier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1501f-535f-4e8c-a63b-d905dec7f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_region_volume_py['ssp126'].sel(year = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d90b5-ae93-4bd1-9a27-ff859d8f3776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
