{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a49e0e6-a595-4c69-9986-92487b128174",
   "metadata": {},
   "source": [
    "## Comparison of GloGEM, PyGEM, and OGGM RGI 01 Volume Outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a803-ff08-4d30-a540-bbd5df8e6233",
   "metadata": {},
   "source": [
    "This notebook imports and processes GloGEM, PyGEM, and OGGM RGI 01 volume outpts. Summing glacial volume change by basin, we produce a plot that compares the three models' projected volume values for each basin by SSP. \n",
    "\n",
    "Last Updated: 25 Nov 2023 | FFW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3bfac-2952-49bf-8005-2023fb4f8681",
   "metadata": {},
   "source": [
    "## Loading in data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa2cdb-7561-40d3-add1-375fd9a533e2",
   "metadata": {},
   "source": [
    "### GloGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcf90b-fb5a-4952-8f6b-37c81b313e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import date\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "## Generic the filepath to the main data folder\n",
    "fpath0 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/'\n",
    "fpath1 = 'Runoff-intercomparison/GloGEM-output/Volume_GloGEM-20230626' \n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_glo = ['BCC-CSM2-MR','CAMS-CSM1-0','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #Use a different path as we have all 5 ssps for volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378cbeb-8bf7-41a6-9336-78fe865ebdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes_13 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes_13[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        temp_df = pd.read_csv(fpath0 + fpath1 +'/CentralAsia/' + model + '/' + SSP  + '/' + 'Volume_CentralAsia.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        # Ensure all indices are same length\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = '13.' + temp_df.index.astype(str)   #Specifying region--need to do for cross region sum\n",
    "        volumes_13[SSP][model] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503163f5-50e9-49b0-addf-b579cb44f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes_14 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes_14[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        temp_df = pd.read_csv(fpath0 + fpath1 +'/SouthAsiaWest/' + model + '/' + SSP  + '/' + 'Volume_SouthAsiaWest.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        # Ensure all indices are same length\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = '14.' + temp_df.index.astype(str)\n",
    "        volumes_14[SSP][model] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d2f91-99fe-4f8c-855d-edfe6843a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes_15 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes_15[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        temp_df = pd.read_csv(fpath0 + fpath1 +'/SouthAsiaEast/' + model + '/' + SSP  + '/' + 'Volume_SouthAsiaEast.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        # Ensure all indices are same length\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = '15.' + temp_df.index.astype(str)\n",
    "        volumes_15[SSP][model] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda5517-e167-4ae4-b8e8-aa45270b469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining regions\n",
    "volumes = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "         volumes[SSP][model] = pd.concat([volumes_13[SSP][model], volumes_14[SSP][model], volumes_15[SSP][model]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fea77-bc8d-4eb0-b44f-0407255d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(1980, 1, 1)\n",
    "end_date = datetime.date(2101, 12, 1)\n",
    "new_indices = pd.date_range(start_date, end_date, freq='A').strftime('%Y-%m').tolist()\n",
    "\n",
    "# Apply new index and datetime conversion\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        volumes[SSP][model].columns = new_indices\n",
    "        volumes[SSP][model].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824c33e-f2d0-42b1-b37d-1948dfe8bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdcf98-6ddb-47bb-a6e0-64c48a5e4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_basin(basin_RGI_list, volume_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [str(x)[-8:] for x in basin_RGI_list]\n",
    "    #runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers within a basin that are not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in volume_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = volume_data.loc[new_basin_list].copy()\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return summed_basin_runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350d14e-2a93-49a5-a6b5-064986afe987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpine_basins = {'YSYK-KOL':'2919', 'TARIM HE':'2914', 'TALAS':'2913', 'LAKE BALKHASH':'2910', \n",
    "    'CHUY':'2905','ARAL SEA':'2902', 'YELLOW RIVER':'2434', 'MEKONG':'2421', 'SALWEEN':'2319', \n",
    "    'INDUS':'2309', 'BRAHMAPUTRA':'2302', 'YANGTZE' : '2433'}\n",
    "\n",
    "basins = ['YSYK-KOL', 'TARIM HE', 'TALAS', 'LAKE BALKHASH', 'CHUY', 'ARAL SEA', 'YELLOW RIVER', 'MEKONG', \n",
    "          'SALWEEN', 'INDUS', 'BRAHMAPUTRA', 'YANGTZE']\n",
    "\n",
    "basin_sums_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_glo[SSP] = {}\n",
    "    for b, basin in enumerate(Alpine_basins):\n",
    "        basin_sums_glo[SSP][basin] = {}\n",
    "        for m, model in enumerate(modelnames_glo):\n",
    "            basin_sums_glo[SSP][basin][model] = sum_basin(select_glaciers_json(Alpine_basins[basin]), volumes[SSP][model]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c158857-c086-4172-a155-0cf7a54e0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate multi GCM means and Quartiles we convert to df then calculate across first axis (GCMs)\n",
    "GCM_mean_glo = {}\n",
    "GCM_q1_glo = {}\n",
    "GCM_q3_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glo[SSP] = {}\n",
    "    GCM_q1_glo[SSP] = {}\n",
    "    GCM_q3_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056ce7a-25f5-4ab1-9c1d-8ee835c32cfb",
   "metadata": {},
   "source": [
    "### PyGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0ec41-d200-4ad3-914c-9e52d4a8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_py = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #List of all SSPs in PyGEM\n",
    "\n",
    "Alpine_basins = {'YSYK-KOL':'2919', 'TARIM HE':'2914', 'TALAS':'2913', 'LAKE BALKHASH':'2910', \n",
    "    'CHUY':'2905','ARAL SEA':'2902', 'YELLOW RIVER':'2434', 'MEKONG':'2421', 'SALWEEN':'2319', \n",
    "    'INDUS':'2309', 'BRAHMAPUTRA':'2302', 'YANGTZE' : '2433'}\n",
    "\n",
    "basins = ['YSYK-KOL', 'TARIM HE', 'TALAS', 'LAKE BALKHASH', 'CHUY', 'ARAL SEA', 'YELLOW RIVER', 'MEKONG', \n",
    "          'SALWEEN', 'INDUS', 'BRAHMAPUTRA', 'YANGTZE']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathPy = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/Runoff-intercomparison/PyGEM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2967b5-7aa8-4093-9c03-3810c90de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_gls = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    basin_gls[basin] = select_glaciers_json(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f30f4-4d2d-4731-9391-658d6193889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all Volume data for RGI 13\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "volume_13 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '13/mass_annual/R13_mass_annual_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_mass_annual.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        volume_13[SSP] = combined_ds * 1e-12              #converting kg to km^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a535ed-564f-4c3e-bd64-2ff83fd91687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all Volume data for RGI 14\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "volume_14 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '14/mass_annual/R14_mass_annual_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_mass_annual.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        volume_14[SSP] = combined_ds * 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f0eac-8a5d-4d27-be28-f61c957c20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all Volume data for RGI 15\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "volume_15 = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '15/mass_annual/R15_mass_annual_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_mass_annual.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        volume_15[SSP] = combined_ds * 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd8d78-a926-4a68-950e-091c641fe7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_ds = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volume_ds[SSP] = xr.concat([volume_13[SSP], volume_14[SSP], volume_15[SSP]], dim='glacier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a7250-2b42-418d-837c-572cd2e61d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting into basins\n",
    "basin_volumes = {}\n",
    "for basin, glacier_list in basin_gls.items():\n",
    "    ## loop over them all, drop the irrelevant IDs, and concatenate the result\n",
    "    basin_volumes[basin] = {}\n",
    "    for s, SSP in enumerate(SSPs):\n",
    "        ds_list = []\n",
    "        try:\n",
    "            ds_filtered = volume_ds[SSP].where(volume_ds[SSP].RGIId.isin(glacier_list), drop=True) \n",
    "            #print(ds_filtered)\n",
    "            ds_list.append(ds_filtered)\n",
    "        except ValueError: ## happens if there are no glaciers from this batch in the selected region\n",
    "            continue\n",
    "        basin_volumes[basin][SSP] = xr.concat(ds_list, dim='glacier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7996f-47c3-4db5-ae5f-ca8af7059282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping indexing (to match other models) and summing basins\n",
    "basin_sums_py = {}\n",
    "for s, SSP in enumerate(SSPs):        \n",
    "    basin_sums_py[SSP] = {}\n",
    "    for basin, glacier_list in basin_gls.items():\n",
    "        basin_sums_py[SSP][basin] = basin_volumes[basin][SSP].sum(dim='glacier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5020d-7507-4ca5-b7c4-7c11a6983292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles\n",
    "GCM_mean_py = {}\n",
    "GCM_q1_py = {}\n",
    "GCM_q3_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_py[SSP] = {}\n",
    "    GCM_q1_py[SSP] = {}\n",
    "    GCM_q3_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_py[SSP][basin] = basin_sums_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.75, dim = 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05646bfc-a8bf-463f-a2c3-689a26ebeeb5",
   "metadata": {},
   "source": [
    "### OGGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a65312-93f8-4642-9be9-5c0a879f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the climate models used\n",
    "modelnames_OG = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5','EC-Earth3', \n",
    "                'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8','INM-CM5-0', \n",
    "                 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM', 'TaiESM1']\n",
    "\n",
    "Alpine_basins = {'YSYK-KOL':'2919', 'TARIM HE':'2914', 'TALAS':'2913', 'LAKE BALKHASH':'2910', \n",
    "    'CHUY':'2905','ARAL SEA':'2902', 'YELLOW RIVER':'2434', 'MEKONG':'2421', 'SALWEEN':'2319', \n",
    "    'INDUS':'2309', 'BRAHMAPUTRA':'2302', 'YANGTZE' : '2433'}\n",
    "\n",
    "# CMCC-CM2-SR5 & TaiESM1 only hold values for ssp585––this is model list without those GCMS\n",
    "modelnames_OG_trimmed = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', \n",
    "                         'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8',\n",
    "                           'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathOG1 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/'\n",
    "fpathOG2 = 'Lizz Research Stuff/Runoff-intercomparison/OGGM/lschuster/runs_2023.3/output/basins/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3292ca-f2a4-41ff-8926-ed580123969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all volume data, OGGM is grouped by basin\n",
    "vol_ds = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    fpath_basin = 'gcm_from_2000_bc_2000_2019/{}/'.format(ID)\n",
    "    #print(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc')\n",
    "    with xr.open_mfdataset(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc') as ds:\n",
    "        ds = ds.volume.load()\n",
    "    vol_ds[basin] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb146f21-50db-434e-9923-92af7243fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summing individual glacier volumes into basin totals and converting m^3 to km^3\n",
    "basin_volume_OG = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    basin_volume_OG[basin] = vol_ds[basin].sum(dim = 'rgi_id') * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334a8c3-4629-46da-ae1f-ce6164dcd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dict of GloPy format\n",
    "basins = ['YSYK-KOL', 'TARIM HE', 'TALAS', 'LAKE BALKHASH', 'CHUY', 'ARAL SEA', 'YELLOW RIVER', 'MEKONG', \n",
    "          'SALWEEN', 'INDUS', 'BRAHMAPUTRA', 'YANGTZE']\n",
    "\n",
    "basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        basin_sums_OG[SSP][basin] = basin_volume_OG[basin].sel(scenario = SSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffed482-8ace-44ec-b183-576daedb3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing GCMs that have no values\n",
    "trimmed_basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    trimmed_basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        trimmed_basin_sums_OG[SSP][basin] = xr.concat([basin_sums_OG[SSP][basin][0:4], basin_sums_OG[SSP][basin][5:-1]], dim='gcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a31c0-7508-462a-bf9f-8ae77c55e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles for OGGM\n",
    "GCM_mean_OG = {}\n",
    "GCM_q1_OG = {}\n",
    "GCM_q3_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    which_ssp = SSPs[s]\n",
    "    GCM_mean_OG[which_ssp] = {}\n",
    "    GCM_q1_OG[which_ssp] = {}\n",
    "    GCM_q3_OG[which_ssp] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.75, dim = 'gcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc47ae1-95f5-40ca-a95b-b6e507c191a0",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aeb11-cb89-4d48-93ae-c55392865f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot setup\n",
    "from cycler import cycler\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['YSYK-KOL', 'TARIM HE', 'TALAS', 'LAKE BALKHASH', 'CHUY', 'ARAL SEA', 'YELLOW RIVER', 'MEKONG', \n",
    "          'SALWEEN', 'INDUS', 'BRAHMAPUTRA', 'YANGTZE']\n",
    "\n",
    "basinstext = ['Ysyk-Kol', 'Tarim He', 'Talas', 'Lake Balkhash', 'Chuy', 'Aral Sea', 'Yellow River', \n",
    "              'Mekong', 'Salween', 'Indus', 'Brahmaputra', 'Yangtze']\n",
    "\n",
    "yrs_glo = np.arange(1980,2101)\n",
    "yrs_glo_dt = pd.to_datetime([str(y)for y in yrs_glo])\n",
    "\n",
    "colors_glo =  plt.colormaps['Greens']\n",
    "line_colors_glo = colors_glo(np.linspace(0.2, 0.6, num = 12))\n",
    "glo_cycler = cycler(color = line_colors_glo)\n",
    "\n",
    "colors_py =  plt.colormaps['Purples']\n",
    "line_colors_py = colors_py(np.linspace(0.2, 0.6,num = 12))\n",
    "py_cycler = cycler(color = line_colors_py)\n",
    "\n",
    "colors_OG =  plt.colormaps['Blues']\n",
    "line_colors_OG = colors_OG(np.linspace(0.2, 0.6,num = 12))\n",
    "OG_cycler = cycler(color = line_colors_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612c864-7857-4f80-bf9c-ed8b43eb3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure all basins contain the same glaciers:\n",
    "num_glac_in_basin_OG = {}\n",
    "num_glac_in_basin_glo = {}\n",
    "num_glac_in_basin_py = {}\n",
    "for b, basin in enumerate(basins):\n",
    "    num_glac_in_basin_OG[basin] = vol_ds[basin].rgi_id.size    #OGGM\n",
    "    num_glac_in_basin_glo[basin] = len(select_glaciers_json(Alpine_basins[basin]))   #GloGEM\n",
    "    num_glac_in_basin_py[basin] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        num_glac_in_basin_py[basin][SSP] = basin_volumes[basin][SSP].RGIId.size     #PyGEM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccefee4-13de-4d4d-84ea-6dcf7671fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 2.4*len(basins)), sharex=True)\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "\n",
    "        #OG won't plot with built-in ds.plot()\n",
    "        #Trim last value as it goes to zero\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], trimmed_basin_sums_OG[which_ssp][basin][:,0:-1].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'dodgerblue', alpha = 0.5)\n",
    "\n",
    "        for m in modelnames_glo:\n",
    "            axs[b, s].plot(yrs_glo_dt[20::], basin_sums_glo[which_ssp][basin][m][20::], color=axs[b, s].set_prop_cycle(glo_cycler), alpha = 0.25)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_mean_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q1_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q3_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20::], GCM_q1_glo[which_ssp][basin][20::], GCM_q3_glo[which_ssp][basin][20::], color = 'green')\n",
    "        axs[b, s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[b,s].plot(yrs_glo_dt[20::], basin_sums_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_mean_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.5)\n",
    "\n",
    "       \n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[b, sub_s].set_xlabel('Year')\n",
    "                axs[b, sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[b, s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            for sub_b in range(len(basins)):\n",
    "                axs[sub_b,s].set_ylabel(basinstext[sub_b]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[b, s].set_ylabel(None)\n",
    "            axs[b, s].set_yticklabels('')\n",
    "\n",
    "for b in range(len(basins)):         \n",
    "    row_min = np.inf\n",
    "    row_max = -np.inf                 \n",
    "    for s in range(len(SSPs)):\n",
    "        data_min = np.min(axs[b, s].get_ybound()[0])\n",
    "        data_max = np.max(axs[b, s].get_ybound()[1])\n",
    "        if data_min < row_min:\n",
    "            row_min = data_min\n",
    "        if data_max > row_max:\n",
    "            row_max = data_max\n",
    "            #row_max[basin] = data_max\n",
    "    for s in range(len(SSPs)):\n",
    "        axs[b, s].set_ylim(row_min, row_max)\n",
    "\n",
    "#Adding in text of # of glaciers in each basin\n",
    "row_max_values = []\n",
    "for b, basin in enumerate(basins):\n",
    "    row_max = max(axs[b, s].get_ylim()[1] for s in range(len(SSPs)))\n",
    "    row_max_values.append(row_max)\n",
    "\n",
    "# for s, SSP in enumerate(scenarios):\n",
    "#     for b, basin in enumerate(basins):\n",
    "#         y_coord = row_max_values[b] * 0.9\n",
    "#         axs[b,s].text(pd.to_datetime('2037-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_glo[basin]}\", fontsize=8)\n",
    "        #axs[b,s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_OG[basin]}\", fontsize=8)\n",
    "        #axs[b,s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_py[basin]['ssp126']}\", fontsize=8)\n",
    "       \n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0,0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, (0.14*len(basins))), ncol=3)\n",
    "\n",
    "plt.suptitle('Glacial Volume Change of Major Alaskan River Basins', x=0.48, y=0.918)\n",
    "plt.title('SSP 126                            SSP 245                           SSP 370                            SSP 585', x=-1.3, y=(1.177* len(basins))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b5f7a-2688-469e-a77e-240d6e94b31d",
   "metadata": {},
   "source": [
    "### Runoff from volume change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2442b7-dd2b-46cc-8774-16cdc9efbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change in volume = runoff so:\n",
    "glacial_melt_glo = {}\n",
    "glacial_melt_OG = {}\n",
    "glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    glacial_melt_glo[SSP] = {}\n",
    "    glacial_melt_OG[SSP] = {}\n",
    "    glacial_melt_py[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        glacial_melt_glo[SSP][basin] = {}                                       #Converting neg volume change to pos runoff\n",
    "        for m, model in enumerate(modelnames_glo):                              #And ice volume to water volume\n",
    "            glacial_melt_glo[SSP][basin][model] = basin_sums_glo[SSP][basin][model][20::].diff()*-0.92\n",
    "        glacial_melt_OG[SSP][basin] = trimmed_basin_sums_OG[SSP][basin].diff(dim = 'time')*-0.92\n",
    "        glacial_melt_py[SSP][basin] = basin_sums_py[SSP][basin].diff(dim = 'year')*-0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c977b6-097d-4123-87fb-5f0637802c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing multi-GCM means and quartiles\n",
    "GCM_mean_glacial_melt_OG = {}\n",
    "GCM_q1_glacial_melt_OG = {}                         #OGGM\n",
    "GCM_q3_glacial_melt_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q1_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q3_glacial_melt_OG[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.75, dim = 'gcm')\n",
    "\n",
    "GCM_mean_glacial_melt_py = {}\n",
    "GCM_q1_glacial_melt_py = {}\n",
    "GCM_q3_glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_py[SSP] = {}\n",
    "    GCM_q1_glacial_melt_py[SSP] = {}                #PyGEM\n",
    "    GCM_q3_glacial_melt_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.75, dim = 'model')\n",
    "\n",
    "GCM_mean_glacial_melt_glo = {}\n",
    "GCM_q1_glacial_melt_glo = {}\n",
    "GCM_q3_glacial_melt_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q1_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q3_glacial_melt_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ba284-1577-476d-8437-db82851e0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 2.4*len(basins)), sharex=True)\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "        \n",
    "        for m in modelnames_glo:\n",
    "            axs[b, s].plot(yrs_glo_dt[20::], glacial_melt_glo[which_ssp][basin][m], color=axs[b, s].set_prop_cycle(glo_cycler), alpha = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_mean_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20::], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'green')\n",
    "        axs[b, s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], glacial_melt_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.2)\n",
    "\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[b,s].plot(yrs_glo_dt[20:-1], glacial_melt_OG[which_ssp][basin].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[b,s].fill_between(yrs_glo_dt[20:-1],  GCM_q1_glacial_melt_OG[which_ssp][basin],  GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'dodgerblue', alpha = 0.25)\n",
    "        \n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[b, sub_s].set_xlabel('Year')\n",
    "                axs[b, sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[b, s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            for sub_b in range(len(basins)):\n",
    "                axs[sub_b,s].set_ylabel(basinstext[sub_b]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[b, s].set_ylabel(None)\n",
    "            axs[b, s].set_yticklabels('')\n",
    "\n",
    "# for b in range(len(basins)):    #To look more closely at inter-quartile range \n",
    "#     row_max = -np.inf\n",
    "#     row_min = np.inf\n",
    "#     data_list = []\n",
    "#     for s in range(len(SSPs)):\n",
    "#         data = np.concatenate([GCM_q3_glacial_melt_py[SSPs[s]][basins[b]][0:-1], GCM_q3_glacial_melt_glo[SSPs[s]][basins[b]], GCM_q3_glacial_melt_OG[SSPs[s]][basins[b]]])\n",
    "#         data_list.extend(data[~np.isnan(data) & np.isfinite(data)])\n",
    "#     if len(data_list) > 0:\n",
    "#         row_max = np.max(data_list)\n",
    "#         row_min = np.min(data_list)\n",
    "#     if row_min >= 0:\n",
    "#         bottom_limit = row_min / 3\n",
    "#     else:\n",
    "#         bottom_limit = row_min * 3\n",
    "#     for s in range(len(SSPs)):\n",
    "#         axs[b, s].set_ylim(bottom_limit, row_max)\n",
    "\n",
    "\n",
    "for b in range(len(basins)):         #To looks at whole picture-limits determined by max/min between all GCMs\n",
    "    row_min = np.inf\n",
    "    row_max = -np.inf\n",
    "    for s in range(len(SSPs)):\n",
    "        data_min = np.min(axs[b, s].get_ybound()[0])\n",
    "        data_max = np.max(axs[b, s].get_ybound()[1])\n",
    "        if data_min < row_min:\n",
    "            row_min = data_min\n",
    "        if data_max > row_max:\n",
    "            row_max = data_max\n",
    "    for s in range(len(SSPs)):\n",
    "        axs[b, s].set_ylim(row_min, row_max)\n",
    "\n",
    "\n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0,0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, 1.74), ncol=3)\n",
    "\n",
    "plt.suptitle('Runoff from Glacial Melt in Major Alaskan River Basins', x=0.52, y=.942)\n",
    "plt.title('SSP 126                                 SSP 245                                SSP 370                                 SSP 585', x=-1.3, y=11.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180b659-3f81-4500-829e-eeceeb01ffad",
   "metadata": {},
   "source": [
    "#### Saving Volume Loss Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbf249-3df8-44c9-9b8a-7967c658102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['YSYK-KOL', 'TARIM HE', 'TALAS', 'LAKE BALKHASH', 'CHUY', 'ARAL SEA', 'YELLOW RIVER', 'MEKONG', \n",
    "          'SALWEEN', 'INDUS', 'BRAHMAPUTRA', 'YANGTZE']\n",
    "\n",
    "modelnames = ['BCC-CSM2-MR', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', \n",
    "                  'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "gmodels = ['GloGEM', 'PyGEM', 'OGGM']\n",
    "\n",
    "volume_loss = {}\n",
    "initial_volume = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    volume_loss[gmodel] = {}\n",
    "    initial_volume[gmodel] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        volume_loss[gmodel][SSP] = {}\n",
    "        initial_volume[gmodel][SSP] = {}\n",
    "        for b, basin in enumerate(basins):\n",
    "            volume_loss[gmodel][SSP][basin] = {}\n",
    "            initial_volume[gmodel][SSP][basin] = {}\n",
    "            for m, GCM in enumerate(modelnames):\n",
    "                if gmodel == 'GloGEM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = basin_sums_glo[SSP][basin][GCM][20] - basin_sums_glo[SSP][basin][GCM][-1]\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = basin_sums_glo[SSP][basin][GCM][20]\n",
    "                if gmodel == 'OGGM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = float(trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[0] - trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[-1])\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = float(trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[0])\n",
    "                if gmodel == 'PyGEM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = float(basin_sums_py[SSP][basin].sel(model = m+1)[0]- basin_sums_py[SSP][basin].sel(model = m+1)[-2])\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = float(basin_sums_py[SSP][basin].sel(model = m+1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf385d-ec01-4038-9fcd-e17b0dc6d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the DataFrames for each SSP and GCM combination\n",
    "volume_loss_dfs = {}\n",
    "initial_volume_dfs = {}\n",
    "\n",
    "# Loop through SSP scenarios\n",
    "for SSP in scenarios:\n",
    "    # Loop through GCMs\n",
    "    for GCM in modelnames:\n",
    "        # Initialize DataFrames for volume loss and initial volume\n",
    "        volume_loss_df = pd.DataFrame(index=basins)\n",
    "        initial_volume_df = pd.DataFrame(index=basins)\n",
    "\n",
    "        # Loop through the glacier models\n",
    "        for gmodel in gmodels:\n",
    "            # Create columns for each model with volume loss and initial volume\n",
    "            volume_loss_df[gmodel] = [volume_loss[gmodel][SSP][basin][GCM] for basin in basins]\n",
    "            initial_volume_df[gmodel] = [initial_volume[gmodel][SSP][basin][GCM] for basin in basins]\n",
    "\n",
    "        # Store the DataFrames in the dictionary with the SSP and GCM as keys\n",
    "        volume_loss_dfs[f\"{SSP}_{GCM}\"] = volume_loss_df\n",
    "        initial_volume_dfs[f\"{SSP}_{GCM}\"] = initial_volume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d475f-47e7-4116-9ac1-63d24a75743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_loss_dfs['ssp126_CESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77aebb-5656-462a-be49-07e7e17efbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Volume Loss/'\n",
    "\n",
    "for key in volume_loss_dfs:\n",
    "    fname = f\"Volume_Loss-RGI-13{key}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    volume_loss_dfs[key].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc941888-a229-47e0-b793-e3ffabe45714",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Initial Volume/'\n",
    "\n",
    "for key in initial_volume_dfs:\n",
    "    fname = f\"Initial_Volume-RGI-13{key}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    initial_volume_dfs[key].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c5163-16bb-4ba3-aa12-4c28a5586a8b",
   "metadata": {},
   "source": [
    "GCM mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8e01d-8b46-4995-aa88-5f67bdee1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_volume_loss = {}\n",
    "mean_initial_volume = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    mean_volume_loss[gmodel] = {}\n",
    "    mean_initial_volume[gmodel] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        mean_volume_loss[gmodel][SSP] = {}\n",
    "        mean_initial_volume[gmodel][SSP] = {}\n",
    "        for b, basin in enumerate(basins):\n",
    "            if gmodel == 'GloGEM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = GCM_mean_glo[SSP][basin][20] - GCM_mean_glo[SSP][basin][-1]\n",
    "                mean_initial_volume[gmodel][SSP][basin] = GCM_mean_glo[SSP][basin][20]\n",
    "            if gmodel == 'OGGM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = float(GCM_mean_OG[SSP][basin][0] - GCM_mean_OG[SSP][basin][-1])\n",
    "                mean_initial_volume[gmodel][SSP][basin] = float(GCM_mean_OG[SSP][basin][0])\n",
    "            if gmodel == 'PyGEM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = float(GCM_mean_py[SSP][basin][0]- GCM_mean_py[SSP][basin][-2])\n",
    "                mean_initial_volume[gmodel][SSP][basin] = float(GCM_mean_py[SSP][basin][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989e574-dc7d-48c2-a86e-fcaa132fb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the DataFrames for each SSP and GCM combination\n",
    "mean_volume_loss_dfs = {}\n",
    "mean_initial_volume_dfs = {}\n",
    "\n",
    "# Loop through SSP scenarios\n",
    "for SSP in scenarios:\n",
    "    # Initialize DataFrames for volume loss and initial volume\n",
    "    mean_volume_loss_df = pd.DataFrame(index=basins)\n",
    "    mean_initial_volume_df = pd.DataFrame(index=basins)\n",
    "\n",
    "    # Loop through the glacier models\n",
    "    for gmodel in gmodels:\n",
    "        # Create columns for each model with volume loss and initial volume\n",
    "        mean_volume_loss_df[gmodel] = [mean_volume_loss[gmodel][SSP][basin] for basin in basins]\n",
    "        mean_initial_volume_df[gmodel] = [mean_initial_volume[gmodel][SSP][basin] for basin in basins]\n",
    "\n",
    "    # Store the DataFrames in the dictionary with the SSP as the key\n",
    "    mean_volume_loss_dfs[SSP] = mean_volume_loss_df\n",
    "    mean_initial_volume_dfs[SSP] = mean_initial_volume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e6b06-117a-40d9-b2f5-996d922fe8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Mean Volumes/'\n",
    "\n",
    "for SSP in scenarios:\n",
    "    fname = f\"Mean_Volume_Loss-RGI-13_{SSP}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    mean_volume_loss_dfs[SSP].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fc8c3-f4a6-4345-ba56-1026cc77fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Mean Volumes/'\n",
    "\n",
    "for SSP in scenarios:\n",
    "    fname = f\"Mean_Initial_Volume-RGI-13{SSP}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    mean_initial_volume_dfs[SSP].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a486e-b494-4869-97f4-4550c730ad03",
   "metadata": {},
   "source": [
    "#### And number of glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d4187-b192-4801-847a-8aac31b7679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glac_list(basin_RGI_list, volume_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [str(x)[-8:] for x in basin_RGI_list]\n",
    "    #runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers within a basin that are not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in volume_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = volume_data.loc[new_basin_list].copy()\n",
    "\n",
    "    glac_list = new_df.index.tolist()\n",
    "    \n",
    "    number_glac = len(glac_list)\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    #summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return glac_list#, number_glac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f9396-602d-4c13-8c02-96a026d0d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making lists of all glaciers in each basin for each model AND counting the totals\n",
    "glac_list = {}\n",
    "number_glac = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    glac_list[gmodel] = {}\n",
    "    number_glac[gmodel] = {}\n",
    "    for b, basin in enumerate(Alpine_basins):\n",
    "        if gmodel == 'GloGEM':\n",
    "            glac_list[gmodel][basin] = get_glac_list(select_glaciers_json(Alpine_basins[basin]), volumes[SSP][model])\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])\n",
    "        if gmodel == 'OGGM':\n",
    "            glac_list[gmodel][basin] = vol_ds[basin].coords['rgi_id'].values.tolist()\n",
    "            glac_list['OGGM'][basin] = [rgi_id[-8:] for rgi_id in glac_list['OGGM'][basin]]\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])\n",
    "        if gmodel == 'PyGEM':    \n",
    "            glac_list[gmodel][basin] = basin_volumes[basin]['ssp126'].coords['RGIId'].values.tolist()\n",
    "            glac_list['PyGEM'][basin] = [rgi_id[-8:] for rgi_id in glac_list['PyGEM'][basin]]\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b07d1-2260-4b01-b985-c5fe23dce3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "for gmodel in gmodels:\n",
    "    for basin in Alpine_basins:\n",
    "        # Calculate the number of glaciers for each model and basin\n",
    "        if gmodel == 'GloGEM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "        elif gmodel == 'OGGM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "        elif gmodel == 'PyGEM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "\n",
    "        # Create a DataFrame for each combination\n",
    "        df = pd.DataFrame({\n",
    "            'Model': gmodel,\n",
    "            'Basin': basin,\n",
    "            'Number_of_Glaciers': [number_of_glaciers]\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "number_glaciers = pd.concat(dfs, ignore_index=True)\n",
    "# Pivot the DataFrame to have the basin names as row indexes and model names as column headers\n",
    "number_glaciers = number_glaciers.pivot(index='Basin', columns='Model', values='Number_of_Glaciers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf56ac-2d5f-4779-935f-614fdbcf99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933cb5b1-9dbb-45c8-94e8-0aae41221ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Number of Glaciers/'\n",
    "\n",
    "fname = \"#glaciers-RGI-13.csv\"\n",
    "\n",
    "# Define the full path of the output file\n",
    "output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "number_glaciers.to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9057a0f-a751-4d85-afd7-197be18f65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_elements(list1, list2):\n",
    "    # Convert the lists to sets for efficient comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find elements that are in set1 but not in set2\n",
    "    unique_in_list1 = list(set1 - set2)\n",
    "\n",
    "    # Find elements that are in set2 but not in set1\n",
    "    unique_in_list2 = list(set2 - set1)\n",
    "\n",
    "    return unique_in_list1, unique_in_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c870c-3d23-4f2f-80df-0fd99c98db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_glaciers = {}\n",
    "for b, basin in enumerate(Alpine_basins):\n",
    "    not_in_oggm, not_in_glo = find_unique_elements(glac_list['GloGEM'][basin], glac_list['OGGM'][basin])\n",
    "\n",
    "    not_in_py = find_unique_elements(glac_list['PyGEM'][basin], glac_list['OGGM'][basin])[1]\n",
    "    \n",
    "    # If both not_in_oggm and not_in_py are empty, set not_in_glo equal to not_in_oggm\n",
    "    if not not_in_oggm and not not_in_py:\n",
    "        not_in_py = not_in_oggm\n",
    "\n",
    "    # Store the results in the dictionary for this basin\n",
    "    missing_glaciers[basin] = {\n",
    "        'Basin': basin,\n",
    "        'GloGEM': not_in_glo,\n",
    "        'OGGM': not_in_oggm,\n",
    "        'PyGEM': not_in_py\n",
    "    }\n",
    "\n",
    "# Concatenate the dictionaries into a single DataFrame\n",
    "missing_glaciers_df = pd.DataFrame(missing_glaciers).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee583ef-a4ad-429b-942f-e585c9bbf69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
