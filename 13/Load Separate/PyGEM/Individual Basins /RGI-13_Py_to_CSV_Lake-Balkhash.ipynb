{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd36cf0-c2f3-46e8-b8fe-dde123da9258",
   "metadata": {},
   "source": [
    "### Aggregation of PyGEM runoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac681a9-ce08-4d24-8f4a-8c62fc648edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import date\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260484b9-f321-4838-91df-21d341f17ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the climate models used\n",
    "modelnames_py = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #List of all SSPs in PyGEM\n",
    "\n",
    "Alpine_basins =  {'LAKE BALKHASH':'2910'}\n",
    "\n",
    "basins = ['LAKE BALKHASH']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathPy = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/Runoff-intercomparison/PyGEM/13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a490d18d-3851-4cdb-8e3e-200f092d9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad6cd16-6629-4849-98b3-05731061c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_gls = {}\n",
    "for basin, code in Alpine_basins.items():\n",
    "    basin_gls[basin] = select_glaciers_json(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66cb9c2-682b-49c7-a410-5e7672496b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all runoff data, taking annual sum, and converting m^3 to km^3\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "rf_ds = {}\n",
    "#annual_rf_ds = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '/R13_runoff_monthly_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_runoff_monthly.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        rf_ds[SSP] = combined_ds\n",
    "        #annual_rf_ds[SSP] = rf_ds[SSP].resample(time='A').sum() * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a283970e-3c3b-4643-b217-a519bc7c8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting into basins\n",
    "#basin_datasets = {}\n",
    "basin_ds_monthly = {}\n",
    "for basin, glacier_list in basin_gls.items():\n",
    "    ## loop over them all, drop the irrelevant IDs, and concatenate the result\n",
    "    #basin_datasets[basin] = {}\n",
    "    basin_ds_monthly[basin] = {}\n",
    "    for s, SSP in enumerate(SSPs):\n",
    "        #ds_list = []\n",
    "        ds_list_monthly = []\n",
    "        try:\n",
    "            #ds_filtered = annual_rf_ds[SSP].where(annual_rf_ds[SSP].RGIId.isin(glacier_list), drop=True)\n",
    "            ds_filtered_monthly = rf_ds[SSP].where(rf_ds[SSP].RGIId.isin(glacier_list), drop=True)\n",
    "            #print(ds_filtered)\n",
    "            #ds_list.append(ds_filtered)\n",
    "            ds_list_monthly.append(ds_filtered_monthly)\n",
    "        except ValueError: ## happens if there are no glaciers from this batch in the selected region\n",
    "            continue\n",
    "        #basin_datasets[basin][SSP] = xr.concat(ds_list, dim='glacier')\n",
    "        basin_ds_monthly[basin][SSP] = xr.concat(ds_list_monthly, dim='glacier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972d59ce-863e-402a-8a56-2801c591211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping indexing (to match other models) and summing basins\n",
    "#basin_sums_py = {}\n",
    "basin_sums_monthly_py = {}\n",
    "for s, SSP in enumerate(SSPs):        \n",
    "    #basin_sums_py[SSP] = {}\n",
    "    basin_sums_monthly_py[SSP] = {}\n",
    "    for basin, glacier_list in basin_gls.items():\n",
    "        #basin_sums_py[SSP][basin] = basin_datasets[basin][SSP].sum(dim='glacier')\n",
    "        basin_sums_monthly_py[SSP][basin] = basin_ds_monthly[basin][SSP].sum(dim='glacier')*1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ee1e5-ae1d-4c39-b7b6-16a87c6139b8",
   "metadata": {},
   "source": [
    "### CSV Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a386ee-0cc0-49ae-9491-cdd56b88cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up filename to reflect what you're writing out, possibly in a nested loop\n",
    "modelnames_all = ['BCC-CSM2-MR', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', \n",
    "                  'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['LAKE BALKHASH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6e6cdc-04e1-4692-a50a-9e456f8524ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(2000, 1, 1)\n",
    "end_date = datetime.date(2100, 12, 1)\n",
    "indices = pd.date_range(start_date, end_date, freq='MS').strftime('%Y-%m').tolist()\n",
    "\n",
    "#Creating dataframes of SSP, basin, and GCM containing all 3 global glacier models\n",
    "out_df_RGI13 = {}\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    out_df_RGI13[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        out_df_RGI13[SSP][basin] = {}\n",
    "        for m, model in enumerate(modelnames_all):\n",
    "            pygem_values = pd.DataFrame(basin_sums_monthly_py[SSP][basin].sel(model=m + 1)).values.flatten()\n",
    "\n",
    "            out_df_RGI13[SSP][basin][model] = pd.DataFrame(\n",
    "                {\n",
    "                    'PyGEM': pygem_values,\n",
    "                    \n",
    "                },\n",
    "                index=indices\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e258b96-71fb-434f-b76b-b720f589d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Yukon data\n",
    "output_dir_RGI13 = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Load Separate/RGI 13/PyGEM/'\n",
    "\n",
    "for SSP in out_df_RGI13:\n",
    "    for basin in out_df_RGI13[SSP]:\n",
    "        for GCM in out_df_RGI13[SSP][basin]:\n",
    "            fnameRGI13 = f\"runoff_{GCM}_{SSP}_{basin}.csv\"\n",
    "\n",
    "            # Define the full path of the output file\n",
    "            output_pathRGI13 = os.path.join(output_dir_RGI13, fnameRGI13)\n",
    "\n",
    "            # Save the DataFrame as CSV\n",
    "            out_df_RGI13[SSP][basin][GCM].to_csv(output_pathRGI13, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd40a43-387f-4918-81ff-9db597515696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
