{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a49e0e6-a595-4c69-9986-92487b128174",
   "metadata": {},
   "source": [
    "## Comparison of GloGEM, PyGEM, and OGGM RGI 08 Volume Outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a803-ff08-4d30-a540-bbd5df8e6233",
   "metadata": {},
   "source": [
    "This notebook imports and processes GloGEM, PyGEM, and OGGM RGI 08 volume outpts. Summing glacial volume change by basin, we produce a plot that compares the three models' projected volume values for each basin by SSP. \n",
    "\n",
    "Last Updated: 19 Oct 2023 | FFW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3bfac-2952-49bf-8005-2023fb4f8681",
   "metadata": {},
   "source": [
    "## Loading in data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa2cdb-7561-40d3-add1-375fd9a533e2",
   "metadata": {},
   "source": [
    "### GloGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcf90b-fb5a-4952-8f6b-37c81b313e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datetime import date\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "## Generic the filepath to the main data folder\n",
    "fpath0 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/'\n",
    "fpath1 = 'Runoff-intercomparison/GloGEM-output/Volume_GloGEM-20230626' \n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_glo = ['BCC-CSM2-MR','CAMS-CSM1-0','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #Use a different path as we have all 5 ssps for volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503163f5-50e9-49b0-addf-b579cb44f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    volumes[SSP] = {}\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        temp_df = pd.read_csv(fpath0 + fpath1 +'/Caucasus/' + model + '/' + SSP  + '/' + 'Volume_Caucasus.dat', sep='\\s+', header=None, skiprows=1, index_col=0)\n",
    "        # Ensure all indices are same length\n",
    "        temp_df.index = temp_df.index.map(lambda x: str(x).zfill(5))\n",
    "        temp_df.index = '12.' + temp_df.index.astype(str)   #Specifying region--need to do for cross region sum\n",
    "        volumes[SSP][model] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fea77-bc8d-4eb0-b44f-0407255d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index using pandas date_range function\n",
    "start_date = datetime.date(1980, 1, 1)\n",
    "end_date = datetime.date(2101, 12, 1)\n",
    "new_indices = pd.date_range(start_date, end_date, freq='A').strftime('%Y-%m').tolist()\n",
    "\n",
    "# Apply new index and datetime conversion\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    for m, model in enumerate(modelnames_glo):\n",
    "        volumes[SSP][model].columns = new_indices\n",
    "        volumes[SSP][model].columns = pd.to_datetime(new_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824c33e-f2d0-42b1-b37d-1948dfe8bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def select_glaciers_json(basin='all'):\n",
    "    '''\n",
    "    Select glaciers within a basin by MRBID from a json-file,\n",
    "    which is stored in the data directory.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    basin: str\n",
    "        String of MRBID or 'all'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    If basin is 'all' a list of all relevant glaciers is returned, for\n",
    "    initiating glacier simulations. If basin is a MRBID the list of glaciers\n",
    "    within that basin is returned.\n",
    "    \n",
    "    Copy of a function written by Erik Holmgren (2022) in holmgren_gha.utils\n",
    "    '''\n",
    "\n",
    "    # fpath = './data/rgi_ids_per_basin.json'\n",
    "    fpath = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/rgi_ids_per_basin.json'  \n",
    "    with open(fpath) as f:\n",
    "        basin_dict = json.load(f)\n",
    "\n",
    "    if basin.lower() != 'all':\n",
    "        glacier_list = basin_dict[basin]\n",
    "    else:\n",
    "        glacier_list = list(itertools.chain.from_iterable(basin_dict.values()))\n",
    "\n",
    "    return glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdcf98-6ddb-47bb-a6e0-64c48a5e4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_basin(basin_RGI_list, volume_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [str(x)[-8:] for x in basin_RGI_list]\n",
    "    #runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers within a basin that are not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in volume_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = volume_data.loc[new_basin_list].copy()\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return summed_basin_runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350d14e-2a93-49a5-a6b5-064986afe987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpine_basins = {'KUBAN':'6223'}\n",
    "\n",
    "basins = ['KUBAN']\n",
    "\n",
    "basin_sums_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        basin_sums_glo[SSP][basin] = {}\n",
    "        for m, model in enumerate(modelnames_glo):\n",
    "            basin_sums_glo[SSP][basin][model] = sum_basin(select_glaciers_json(Alpine_basins[basin]), volumes[SSP][model]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c158857-c086-4172-a155-0cf7a54e0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate multi GCM means and Quartiles we convert to df then calculate across first axis (GCMs)\n",
    "GCM_mean_glo = {}\n",
    "GCM_q1_glo = {}\n",
    "GCM_q3_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glo[SSP] = {}\n",
    "    GCM_q1_glo[SSP] = {}\n",
    "    GCM_q3_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glo[SSP][basin] = pd.DataFrame(basin_sums_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056ce7a-25f5-4ab1-9c1d-8ee835c32cfb",
   "metadata": {},
   "source": [
    "### PyGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0ec41-d200-4ad3-914c-9e52d4a8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "#All of the climate models used\n",
    "modelnames_py = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "SSPs = ['ssp126','ssp245','ssp370','ssp585'] #List of all SSPs in PyGEM\n",
    "\n",
    "Alpine_basins = {'KUBAN':'6223'}\n",
    "\n",
    "basins = ['KUBAN']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathPy = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/Lizz Research Stuff/Runoff-intercomparison/PyGEM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2967b5-7aa8-4093-9c03-3810c90de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_gls = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    basin_gls[basin] = select_glaciers_json(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a535ed-564f-4c3e-bd64-2ff83fd91687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all Volume data for RGI 02\n",
    "import glob   #use glob to group files by filename similarities (in this case, SSP)\n",
    "\n",
    "volume_ds = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    fpath1 = '12/mass_annual/R12_mass_annual_c2_ba1_1set_2000_2100-{}'.format(SSP)\n",
    "    file_pattern = f'{fpathPy + fpath1}*.nc'\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    #print(file_list)\n",
    "    \n",
    "    datasets = []  # Create an empty list for each SSP\n",
    "    if file_list:\n",
    "        for file in file_list:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                ds = ds.glac_mass_annual.load()\n",
    "                datasets.append(ds)\n",
    "    \n",
    "        combined_ds = xr.concat(datasets, dim='glacier')  # Concatenate the datasets\n",
    "        volume_ds[SSP] = combined_ds * 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a7250-2b42-418d-837c-572cd2e61d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting into basins\n",
    "basin_volumes = {}\n",
    "for basin, glacier_list in basin_gls.items():\n",
    "    ## loop over them all, drop the irrelevant IDs, and concatenate the result\n",
    "    basin_volumes[basin] = {}\n",
    "    for s, SSP in enumerate(SSPs):\n",
    "        ds_list = []\n",
    "        try:\n",
    "            ds_filtered = volume_ds[SSP].where(volume_ds[SSP].RGIId.isin(glacier_list), drop=True) \n",
    "            #print(ds_filtered)\n",
    "            ds_list.append(ds_filtered)\n",
    "        except ValueError: ## happens if there are no glaciers from this batch in the selected region\n",
    "            continue\n",
    "        basin_volumes[basin][SSP] = xr.concat(ds_list, dim='glacier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7996f-47c3-4db5-ae5f-ca8af7059282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flipping indexing (to match other models) and summing basins\n",
    "basin_sums_py = {}\n",
    "for s, SSP in enumerate(SSPs):        \n",
    "    basin_sums_py[SSP] = {}\n",
    "    for basin, glacier_list in basin_gls.items():\n",
    "        basin_sums_py[SSP][basin] = basin_volumes[basin][SSP].sum(dim='glacier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5020d-7507-4ca5-b7c4-7c11a6983292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles\n",
    "GCM_mean_py = {}\n",
    "GCM_q1_py = {}\n",
    "GCM_q3_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_py[SSP] = {}\n",
    "    GCM_q1_py[SSP] = {}\n",
    "    GCM_q3_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_py[SSP][basin] = basin_sums_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_py[SSP][basin] = basin_sums_py[SSP][basin].quantile(q = 0.75, dim = 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05646bfc-a8bf-463f-a2c3-689a26ebeeb5",
   "metadata": {},
   "source": [
    "### OGGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a65312-93f8-4642-9be9-5c0a879f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the climate models used\n",
    "modelnames_OG = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'CMCC-CM2-SR5','EC-Earth3', \n",
    "                'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8','INM-CM5-0', \n",
    "                 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM', 'TaiESM1']\n",
    "\n",
    "Alpine_basins = {'KUBAN':'6223'}\n",
    "\n",
    "basins = ['KUBAN']\n",
    "\n",
    "# CMCC-CM2-SR5 & TaiESM1 only hold values for ssp585––this is model list without those GCMS\n",
    "modelnames_OG_trimmed = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', \n",
    "                         'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', 'INM-CM4-8',\n",
    "                           'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "#Generic filepath to navigate to Drive folder \n",
    "fpathOG1 = '/Users/finnwimberly/Library/CloudStorage/GoogleDrive-fwimberly@middlebury.edu/My Drive/'\n",
    "fpathOG2 = 'Lizz Research Stuff/Runoff-intercomparison/OGGM/lschuster/runs_2023.3/output/basins/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3292ca-f2a4-41ff-8926-ed580123969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all runoff data, OGGM is grouped by basin\n",
    "vol_ds = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    fpath_basin = 'gcm_from_2000_bc_2000_2019/{}/'.format(ID)\n",
    "    #print(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc')\n",
    "    with xr.open_mfdataset(f'{fpathOG1 + fpathOG2 + fpath_basin}*.nc') as ds:\n",
    "        ds = ds.volume.load()\n",
    "    vol_ds[basin] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb146f21-50db-434e-9923-92af7243fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summing individual glacier runoff into basin totals and converting m^3 to km^3\n",
    "basin_volume_OG = {}\n",
    "for basin, ID in Alpine_basins.items():\n",
    "    basin_volume_OG[basin] = vol_ds[basin].sum(dim = 'rgi_id') * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334a8c3-4629-46da-ae1f-ce6164dcd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dict of GloPy format\n",
    "basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        basin_sums_OG[SSP][basin] = basin_volume_OG[basin].sel(scenario = SSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffed482-8ace-44ec-b183-576daedb3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing GCMs that have no values\n",
    "trimmed_basin_sums_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    trimmed_basin_sums_OG[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        trimmed_basin_sums_OG[SSP][basin] = xr.concat([basin_sums_OG[SSP][basin][0:4], basin_sums_OG[SSP][basin][5:-1]], dim='gcm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a31c0-7508-462a-bf9f-8ae77c55e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute multi GCM means and quartiles for OGGM\n",
    "GCM_mean_OG = {}\n",
    "GCM_q1_OG = {}\n",
    "GCM_q3_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    which_ssp = SSPs[s]\n",
    "    GCM_mean_OG[which_ssp] = {}\n",
    "    GCM_q1_OG[which_ssp] = {}\n",
    "    GCM_q3_OG[which_ssp] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_OG[which_ssp][basin] = trimmed_basin_sums_OG[which_ssp][basin].quantile(q = 0.75, dim = 'gcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc47ae1-95f5-40ca-a95b-b6e507c191a0",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aeb11-cb89-4d48-93ae-c55392865f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot setup\n",
    "from cycler import cycler\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['KUBAN']\n",
    "\n",
    "basinstext = ['Kuban']\n",
    "\n",
    "yrs_glo = np.arange(1980,2101)\n",
    "yrs_glo_dt = pd.to_datetime([str(y)for y in yrs_glo])\n",
    "\n",
    "colors_glo =  plt.colormaps['Greens']\n",
    "line_colors_glo = colors_glo(np.linspace(0.2, 0.6, num = 12))\n",
    "glo_cycler = cycler(color = line_colors_glo)\n",
    "\n",
    "colors_py =  plt.colormaps['Purples']\n",
    "line_colors_py = colors_py(np.linspace(0.2, 0.6,num = 12))\n",
    "py_cycler = cycler(color = line_colors_py)\n",
    "\n",
    "colors_OG =  plt.colormaps['Blues']\n",
    "line_colors_OG = colors_OG(np.linspace(0.2, 0.6,num = 12))\n",
    "OG_cycler = cycler(color = line_colors_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccefee4-13de-4d4d-84ea-6dcf7671fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 2.4*len(basins)), sharex=True)\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "\n",
    "        #OG won't plot with built-in ds.plot()\n",
    "        #Trim last value as it goes to zero\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[s].plot(yrs_glo_dt[20:-1], trimmed_basin_sums_OG[which_ssp][basin][:,0:-1].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_mean_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20:-1], GCM_q1_OG[which_ssp][basin][0:-1], GCM_q3_OG[which_ssp][basin][0:-1], color = 'dodgerblue', alpha = 0.5)\n",
    "\n",
    "        for m in modelnames_glo:\n",
    "            axs[s].plot(yrs_glo_dt[20::], basin_sums_glo[which_ssp][basin][m][20::], color=axs[s].set_prop_cycle(glo_cycler), alpha = 0.25)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_mean_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q1_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q3_glo[which_ssp][basin][20::], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20::], GCM_q1_glo[which_ssp][basin][20::], GCM_q3_glo[which_ssp][basin][20::], color = 'green')\n",
    "        axs[s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[s].plot(yrs_glo_dt[20::], basin_sums_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_mean_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20::], GCM_q1_py[which_ssp][basin][0:-1], GCM_q3_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.5)\n",
    "\n",
    "       \n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[sub_s].set_xlabel('Year')\n",
    "                axs[sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            axs[s].set_ylabel(basinstext[b]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[s].set_ylabel(None)\n",
    "            axs[s].set_yticklabels('')\n",
    "        \n",
    "row_min = np.inf\n",
    "row_max = -np.inf                 \n",
    "for s in range(len(SSPs)):\n",
    "    data_min = np.min(axs[s].get_ybound()[0])\n",
    "    data_max = np.max(axs[s].get_ybound()[1])\n",
    "    if data_min < row_min:\n",
    "        row_min = data_min\n",
    "    if data_max > row_max:\n",
    "        row_max = data_max\n",
    "        #row_max[basin] = data_max\n",
    "for s in range(len(SSPs)):\n",
    "    axs[s].set_ylim(row_min, row_max)\n",
    "\n",
    "#Adding in text of # of glaciers in each basin\n",
    "row_max_values = []\n",
    "for b, basin in enumerate(basins):\n",
    "    row_max = max(axs[s].get_ylim()[1] for s in range(len(SSPs)))\n",
    "    row_max_values.append(row_max)\n",
    "\n",
    "# for s, SSP in enumerate(scenarios):\n",
    "#     for b, basin in enumerate(basins):\n",
    "#         y_coord = row_max_values[b] * 0.9\n",
    "#         axs[s].text(pd.to_datetime('2037-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_glo[basin]}\", fontsize=8)\n",
    "        #axs[s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_OG[basin]}\", fontsize=8)\n",
    "        #axs[s].text(pd.to_datetime('2040-01-01'), y_coord,  f\"# glaciers = {num_glac_in_basin_py[basin]['ssp126']}\", fontsize=8)\n",
    "       \n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, 1.32), ncol=3)\n",
    "\n",
    "plt.suptitle('Composite Glacier Volume Change of Major Caucas River Basins', x=0.48, y=1.19)\n",
    "plt.title('SSP 126                            SSP 245                           SSP 370                            SSP 585', x=-1.3, y=(1.0* len(basins))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b5f7a-2688-469e-a77e-240d6e94b31d",
   "metadata": {},
   "source": [
    "### Runoff from volume change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2442b7-dd2b-46cc-8774-16cdc9efbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change in volume = runoff so:\n",
    "glacial_melt_glo = {}\n",
    "glacial_melt_OG = {}\n",
    "glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    glacial_melt_glo[SSP] = {}\n",
    "    glacial_melt_OG[SSP] = {}\n",
    "    glacial_melt_py[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        glacial_melt_glo[SSP][basin] = {}                                       #Converting neg volume change to pos runoff\n",
    "        for m, model in enumerate(modelnames_glo):                              #And ice volume to water volume\n",
    "            glacial_melt_glo[SSP][basin][model] = basin_sums_glo[SSP][basin][model][20::].diff()*-0.92\n",
    "        glacial_melt_OG[SSP][basin] = trimmed_basin_sums_OG[SSP][basin].diff(dim = 'time')*-0.92\n",
    "        glacial_melt_py[SSP][basin] = basin_sums_py[SSP][basin].diff(dim = 'year')*-0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c977b6-097d-4123-87fb-5f0637802c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing multi-GCM means and quartiles\n",
    "GCM_mean_glacial_melt_OG = {}\n",
    "GCM_q1_glacial_melt_OG = {}                         #OGGM\n",
    "GCM_q3_glacial_melt_OG = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q1_glacial_melt_OG[SSP] = {}\n",
    "    GCM_q3_glacial_melt_OG[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].mean(dim = 'gcm')\n",
    "        GCM_q1_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.25, dim = 'gcm')\n",
    "        GCM_q3_glacial_melt_OG[SSP][basin] = glacial_melt_OG[SSP][basin].quantile(q = 0.75, dim = 'gcm')\n",
    "\n",
    "GCM_mean_glacial_melt_py = {}\n",
    "GCM_q1_glacial_melt_py = {}\n",
    "GCM_q3_glacial_melt_py = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_py[SSP] = {}\n",
    "    GCM_q1_glacial_melt_py[SSP] = {}                #PyGEM\n",
    "    GCM_q3_glacial_melt_py[SSP] = {}\n",
    "    for basin in basins:\n",
    "        GCM_mean_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].mean(dim = 'model')\n",
    "        GCM_q1_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.25, dim = 'model')\n",
    "        GCM_q3_glacial_melt_py[SSP][basin] = glacial_melt_py[SSP][basin].quantile(q = 0.75, dim = 'model')\n",
    "\n",
    "GCM_mean_glacial_melt_glo = {}\n",
    "GCM_q1_glacial_melt_glo = {}\n",
    "GCM_q3_glacial_melt_glo = {}\n",
    "for s, SSP in enumerate(SSPs):\n",
    "    GCM_mean_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q1_glacial_melt_glo[SSP] = {}\n",
    "    GCM_q3_glacial_melt_glo[SSP] = {}\n",
    "    for b, basin in enumerate(basins):\n",
    "        GCM_mean_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).mean(axis=1)\n",
    "        GCM_q1_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.25, axis=1)\n",
    "        GCM_q3_glacial_melt_glo[SSP][basin] = pd.DataFrame(glacial_melt_glo[SSP][basin]).quantile(q=0.75, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ba284-1577-476d-8437-db82851e0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all data\n",
    "fig, axs = plt.subplots(len(basins), len(SSPs), figsize=(10, 2.4*len(basins)), sharex=True)\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    which_ssp = SSPs[s]\n",
    "    for b, basin in enumerate(basins):\n",
    "        \n",
    "        for m in modelnames_glo:\n",
    "            axs[s].plot(yrs_glo_dt[20::], glacial_melt_glo[which_ssp][basin][m], color=axs[s].set_prop_cycle(glo_cycler), alpha = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_mean_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20::], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'darkgreen', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20::], GCM_q1_glacial_melt_glo[which_ssp][basin], GCM_q3_glacial_melt_glo[which_ssp][basin], color = 'green')\n",
    "        axs[s].set(xlim=(pd.to_datetime('2000-01-01'), pd.to_datetime('2100-01-01')))\n",
    "\n",
    "        for m, model in enumerate(modelnames_py):\n",
    "            axs[s].plot(yrs_glo_dt[20:-1], glacial_melt_py[which_ssp][basin].sel(model = m+1)[0:-1], color = 'purple', alpha = 0.15)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_py[which_ssp][basin][0:-1], GCM_q3_glacial_melt_py[which_ssp][basin][0:-1], color = 'purple', alpha = 0.2)\n",
    "\n",
    "        for m, model in enumerate(modelnames_OG_trimmed):\n",
    "            axs[s].plot(yrs_glo_dt[20:-1], glacial_melt_OG[which_ssp][basin].sel(gcm = modelnames_OG_trimmed[m]), color = 'dodgerblue', alpha = 0.15)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_mean_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.9)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q1_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[s].plot(yrs_glo_dt[20:-1], GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'royalblue', linewidth = 0.4)\n",
    "        axs[s].fill_between(yrs_glo_dt[20:-1],  GCM_q1_glacial_melt_OG[which_ssp][basin],  GCM_q3_glacial_melt_OG[which_ssp][basin], color = 'dodgerblue', alpha = 0.25)\n",
    "        \n",
    "        #Setting x and y labels and making y limits uniform within basins\n",
    "        if b == (len(basins)-1):\n",
    "            for sub_s in range(4):  # Use a different variable name for the inner loop\n",
    "                axs[sub_s].set_xlabel('Year')\n",
    "                axs[sub_s].set_xticks([pd.to_datetime('2025'),pd.to_datetime('2050'), pd.to_datetime('2075')], [2025, 2050, 2075])\n",
    "        else:\n",
    "            axs[s].set_xlabel(None) \n",
    "        \n",
    "        if s == 0:                                                                    #Setting basin labels\n",
    "            axs[s].set_ylabel(basinstext[0]+ r' $[km^3]$')\n",
    "        if s != 0:\n",
    "            axs[s].set_ylabel(None)\n",
    "            axs[s].set_yticklabels('')\n",
    "\n",
    "# for b in range(len(basins)):    #To look more closely at inter-quartile range \n",
    "#     row_max = -np.inf\n",
    "#     row_min = np.inf\n",
    "#     data_list = []\n",
    "#     for s in range(len(SSPs)):\n",
    "#         data = np.concatenate([GCM_q3_glacial_melt_py[SSPs[s]][basins[b]][0:-1], GCM_q3_glacial_melt_glo[SSPs[s]][basins[b]], GCM_q3_glacial_melt_OG[SSPs[s]][basins[b]]])\n",
    "#         data_list.extend(data[~np.isnan(data) & np.isfinite(data)])\n",
    "#     if len(data_list) > 0:\n",
    "#         row_max = np.max(data_list)\n",
    "#         row_min = np.min(data_list)\n",
    "#     if row_min >= 0:\n",
    "#         bottom_limit = row_min / 3\n",
    "#     else:\n",
    "#         bottom_limit = row_min * 3\n",
    "#     for s in range(len(SSPs)):\n",
    "#         axs[b, s].set_ylim(bottom_limit, row_max)\n",
    "\n",
    "\n",
    "for b in range(len(basins)):         #To looks at whole picture-limits determined by max/min between all GCMs\n",
    "    row_min = np.inf\n",
    "    row_max = -np.inf\n",
    "    for s in range(len(SSPs)):\n",
    "        data_min = np.min(axs[s].get_ybound()[0])\n",
    "        data_max = np.max(axs[s].get_ybound()[1])\n",
    "        if data_min < row_min:\n",
    "            row_min = data_min\n",
    "        if data_max > row_max:\n",
    "            row_max = data_max\n",
    "    for s in range(len(SSPs)):\n",
    "        axs[s].set_ylim(row_min, row_max)\n",
    "\n",
    "\n",
    "green_patch = mpatches.Patch(color='darkgreen', label='GloGEM')\n",
    "purple_patch = mpatches.Patch(color='purple', label='PyGEM') \n",
    "blue_patch = mpatches.Patch(color='royalblue', label='OGGM')\n",
    "axs[0].legend(handles=[green_patch, purple_patch, blue_patch], bbox_to_anchor=(3.15, 1.34), ncol=3)\n",
    "\n",
    "plt.suptitle('Runoff from Glacial Melt in Major Caucus River Basins', x=0.5, y=1.19)\n",
    "plt.title('SSP 126                                 SSP 245                                SSP 370                                 SSP 585', x=-1.3, y=(1.0* len(basins))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a8ad7-ed51-4ff3-9cbb-b603b151bea3",
   "metadata": {},
   "source": [
    "### CSV Readout\n",
    "Going to use initial glacial volume in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddbd01-764b-4c23-8e69-e82cfae94ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames_all = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','EC-Earth3','EC-Earth3-Veg','FGOALS-f3-L','GFDL-ESM4',\n",
    "              'INM-CM4-8','INM-CM5-0','MPI-ESM1-2-HR','MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "out_df = {}\n",
    "\n",
    "for s, SSP in enumerate(scenarios):\n",
    "    out_df[SSP] = {}\n",
    "\n",
    "    for m, GCM in enumerate(modelnames_all):\n",
    "        basin_data = {}  # Store data for each basin\n",
    "        for b, basin in enumerate(basins):\n",
    "            glo_values = basin_sums_glo[SSP][basin][GCM][20]\n",
    "            pygem_values = basin_sums_py[SSP][basin].sel(model=m + 1)[0].values\n",
    "            oggm_values = trimmed_basin_sums_OG[SSP][basin].sel(gcm=modelnames_all[m])[0].values\n",
    "\n",
    "            # Create a dictionary with values for the current basin\n",
    "            data = {\n",
    "                'GloGEM': [glo_values],  # Wrap scalar values in a list\n",
    "                'OGGM': [oggm_values],\n",
    "                'PyGEM': [pygem_values],\n",
    "            }\n",
    "\n",
    "            # Create a DataFrame for the current basin\n",
    "            basin_df = pd.DataFrame(data)\n",
    "            basin_df.index = [basin]  # Set the index as 'Basin' and wrap it in a list\n",
    "\n",
    "            # Store the basin's DataFrame in the basin_data dictionary\n",
    "            basin_data[basin] = basin_df\n",
    "\n",
    "        # Concatenate all basin DataFrames into a single DataFrame for this model and scenario\n",
    "        out_df[SSP][GCM] = pd.concat(basin_data.values(), axis=0)\n",
    "        out_df[SSP][GCM].index.name = 'Basin'  # Set the index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56c5d4-379a-4421-8598-1c7739dafb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the CSV files\n",
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Volume/RGI 12/'\n",
    "\n",
    "for SSP in out_df:\n",
    "    for GCM in out_df[SSP]:\n",
    "        fname = f\"InitialVolume{GCM}_{SSP}.csv\"\n",
    "\n",
    "        # Define the full path of the output file\n",
    "        output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "        # Save the DataFrame as CSV\n",
    "        out_df[SSP][GCM].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5f313-7b80-4789-80ef-b4ea2efecfc0",
   "metadata": {},
   "source": [
    "#### Saving Volume Loss Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b94ea-e78a-4b72-b4ab-8a0244bd84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ['ssp126','ssp245','ssp370','ssp585']\n",
    "\n",
    "basins = ['KUBAN']\n",
    "\n",
    "modelnames = ['BCC-CSM2-MR', 'CESM2', 'CESM2-WACCM', 'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4', \n",
    "                  'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n",
    "\n",
    "gmodels = ['GloGEM', 'PyGEM', 'OGGM']\n",
    "\n",
    "volume_loss = {}\n",
    "initial_volume = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    volume_loss[gmodel] = {}\n",
    "    initial_volume[gmodel] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        volume_loss[gmodel][SSP] = {}\n",
    "        initial_volume[gmodel][SSP] = {}\n",
    "        for b, basin in enumerate(basins):\n",
    "            volume_loss[gmodel][SSP][basin] = {}\n",
    "            initial_volume[gmodel][SSP][basin] = {}\n",
    "            for m, GCM in enumerate(modelnames):\n",
    "                if gmodel == 'GloGEM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = basin_sums_glo[SSP][basin][GCM][20] - basin_sums_glo[SSP][basin][GCM][-1]\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = basin_sums_glo[SSP][basin][GCM][20]\n",
    "                if gmodel == 'OGGM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = float(trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[0] - trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[-1])\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = float(trimmed_basin_sums_OG[SSP][basin].sel(gcm = GCM)[0])\n",
    "                if gmodel == 'PyGEM':\n",
    "                    volume_loss[gmodel][SSP][basin][GCM] = float(basin_sums_py[SSP][basin].sel(model = m+1)[0]- basin_sums_py[SSP][basin].sel(model = m+1)[-2])\n",
    "                    initial_volume[gmodel][SSP][basin][GCM] = float(basin_sums_py[SSP][basin].sel(model = m+1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a603ce6-bf76-4ee5-b618-0beb0110c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the DataFrames for each SSP and GCM combination\n",
    "volume_loss_dfs = {}\n",
    "initial_volume_dfs = {}\n",
    "\n",
    "# Loop through SSP scenarios\n",
    "for SSP in scenarios:\n",
    "    # Loop through GCMs\n",
    "    for GCM in modelnames:\n",
    "        # Initialize DataFrames for volume loss and initial volume\n",
    "        volume_loss_df = pd.DataFrame(index=basins)\n",
    "        initial_volume_df = pd.DataFrame(index=basins)\n",
    "\n",
    "        # Loop through the glacier models\n",
    "        for gmodel in gmodels:\n",
    "            # Create columns for each model with volume loss and initial volume\n",
    "            volume_loss_df[gmodel] = [volume_loss[gmodel][SSP][basin][GCM] for basin in basins]\n",
    "            initial_volume_df[gmodel] = [initial_volume[gmodel][SSP][basin][GCM] for basin in basins]\n",
    "\n",
    "        # Store the DataFrames in the dictionary with the SSP and GCM as keys\n",
    "        volume_loss_dfs[f\"{SSP}_{GCM}\"] = volume_loss_df\n",
    "        initial_volume_dfs[f\"{SSP}_{GCM}\"] = initial_volume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cc140-1791-4453-87fa-48f6d25c5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_loss_dfs['ssp126_CESM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb219b-7ce0-452d-9661-a1b48983d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Volume Loss/'\n",
    "\n",
    "for key in volume_loss_dfs:\n",
    "    fname = f\"Volume_Loss-RGI-12{key}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    volume_loss_dfs[key].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d8190-37dc-4fe9-b87f-b87835b53848",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Initial Volume/'\n",
    "\n",
    "for key in initial_volume_dfs:\n",
    "    fname = f\"Initial_Volume-RGI-12{key}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    initial_volume_dfs[key].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f1844-0286-4499-812d-8ed80c3a0aae",
   "metadata": {},
   "source": [
    "GCM mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc086b9d-15ef-4fec-adc5-28ac67630529",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_volume_loss = {}\n",
    "mean_initial_volume = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    mean_volume_loss[gmodel] = {}\n",
    "    mean_initial_volume[gmodel] = {}\n",
    "    for s, SSP in enumerate(scenarios):\n",
    "        mean_volume_loss[gmodel][SSP] = {}\n",
    "        mean_initial_volume[gmodel][SSP] = {}\n",
    "        for b, basin in enumerate(basins):\n",
    "            if gmodel == 'GloGEM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = GCM_mean_glo[SSP][basin][20] - GCM_mean_glo[SSP][basin][-1]\n",
    "                mean_initial_volume[gmodel][SSP][basin] = GCM_mean_glo[SSP][basin][20]\n",
    "            if gmodel == 'OGGM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = float(GCM_mean_OG[SSP][basin][0] - GCM_mean_OG[SSP][basin][-1])\n",
    "                mean_initial_volume[gmodel][SSP][basin] = float(GCM_mean_OG[SSP][basin][0])\n",
    "            if gmodel == 'PyGEM':\n",
    "                mean_volume_loss[gmodel][SSP][basin] = float(GCM_mean_py[SSP][basin][0]- GCM_mean_py[SSP][basin][-2])\n",
    "                mean_initial_volume[gmodel][SSP][basin] = float(GCM_mean_py[SSP][basin][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edddd6-b0ff-4c7c-8d65-ac3ce017adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the DataFrames for each SSP and GCM combination\n",
    "mean_volume_loss_dfs = {}\n",
    "mean_initial_volume_dfs = {}\n",
    "\n",
    "# Loop through SSP scenarios\n",
    "for SSP in scenarios:\n",
    "    # Initialize DataFrames for volume loss and initial volume\n",
    "    mean_volume_loss_df = pd.DataFrame(index=basins)\n",
    "    mean_initial_volume_df = pd.DataFrame(index=basins)\n",
    "\n",
    "    # Loop through the glacier models\n",
    "    for gmodel in gmodels:\n",
    "        # Create columns for each model with volume loss and initial volume\n",
    "        mean_volume_loss_df[gmodel] = [mean_volume_loss[gmodel][SSP][basin] for basin in basins]\n",
    "        mean_initial_volume_df[gmodel] = [mean_initial_volume[gmodel][SSP][basin] for basin in basins]\n",
    "\n",
    "    # Store the DataFrames in the dictionary with the SSP as the key\n",
    "    mean_volume_loss_dfs[SSP] = mean_volume_loss_df\n",
    "    mean_initial_volume_dfs[SSP] = mean_initial_volume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fa232-9ffb-4e51-b20e-7c961bb7854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Mean Volumes/'\n",
    "\n",
    "for SSP in scenarios:\n",
    "    fname = f\"Mean_Volume_Loss-RGI-12_{SSP}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    mean_volume_loss_dfs[SSP].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd684bd-6e64-422d-961e-7505152da9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Mean Volumes/'\n",
    "\n",
    "for SSP in scenarios:\n",
    "    fname = f\"Mean_Initial_Volume-RGI-12{SSP}.csv\"\n",
    "\n",
    "    # Define the full path of the output file\n",
    "    output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    mean_initial_volume_dfs[SSP].to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27b875-331b-44bc-bada-aca7c80de9c3",
   "metadata": {},
   "source": [
    "#### And number of glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c630d-ed73-469f-8c5e-16ff27e1f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glac_list(basin_RGI_list, volume_data):\n",
    "    # Create new list to match our RGI formatting\n",
    "    new_basin_list = [str(x)[-8:] for x in basin_RGI_list]\n",
    "    #runoff_data = runoff_data.transpose()\n",
    "    \n",
    "    #TODO: create list of glaciers within a basin that are not included in GloGEM output\n",
    "    # Filter new_basin_list to keep only the indexes present in the DataFrame\n",
    "    new_basin_list = [x for x in new_basin_list if x in volume_data.index]\n",
    "    \n",
    "    # Extract glaciers contained in the list from original df and create a new df\n",
    "    new_df = volume_data.loc[new_basin_list].copy()\n",
    "\n",
    "    glac_list = new_df.index.tolist()\n",
    "    \n",
    "    number_glac = len(glac_list)\n",
    "    \n",
    "    # Sum the values of the glaciers within the basin\n",
    "    #summed_basin_runoff = new_df.sum()\n",
    "    #print(summed_basin_runoff)\n",
    "    \n",
    "    return glac_list#, number_glac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94589a10-35fd-45ba-9744-3e73448c0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making lists of all glaciers in each basin for each model AND counting the totals\n",
    "glac_list = {}\n",
    "number_glac = {}\n",
    "for g, gmodel in enumerate(gmodels):\n",
    "    glac_list[gmodel] = {}\n",
    "    number_glac[gmodel] = {}\n",
    "    for b, basin in enumerate(Alpine_basins):\n",
    "        if gmodel == 'GloGEM':\n",
    "            glac_list[gmodel][basin] = get_glac_list(select_glaciers_json(Alpine_basins[basin]), volumes[SSP][model])\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])\n",
    "        if gmodel == 'OGGM':\n",
    "            glac_list[gmodel][basin] = vol_ds[basin].coords['rgi_id'].values.tolist()\n",
    "            glac_list['OGGM'][basin] = [rgi_id[-8:] for rgi_id in glac_list['OGGM'][basin]]\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])\n",
    "        if gmodel == 'PyGEM':    \n",
    "            glac_list[gmodel][basin] = basin_volumes[basin]['ssp126'].coords['RGIId'].values.tolist()\n",
    "            glac_list['PyGEM'][basin] = [rgi_id[-8:] for rgi_id in glac_list['PyGEM'][basin]]\n",
    "            number_glac[gmodel][basin] = len(glac_list[gmodel][basin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4392a-9870-4ed7-a564-78523f453208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "for gmodel in gmodels:\n",
    "    for basin in Alpine_basins:\n",
    "        # Calculate the number of glaciers for each model and basin\n",
    "        if gmodel == 'GloGEM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "        elif gmodel == 'OGGM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "        elif gmodel == 'PyGEM':\n",
    "            number_of_glaciers = len(glac_list[gmodel][basin])\n",
    "\n",
    "        # Create a DataFrame for each combination\n",
    "        df = pd.DataFrame({\n",
    "            'Model': gmodel,\n",
    "            'Basin': basin,\n",
    "            'Number_of_Glaciers': [number_of_glaciers]\n",
    "        })\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "number_glaciers = pd.concat(dfs, ignore_index=True)\n",
    "# Pivot the DataFrame to have the basin names as row indexes and model names as column headers\n",
    "number_glaciers = number_glaciers.pivot(index='Basin', columns='Model', values='Number_of_Glaciers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12da86-2525-4507-8dd3-7470645c2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f1eda-5d62-4786-832b-16a6ed262eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/finnwimberly/Desktop/Lizz Research/CSV Outputs/Parameters/Regional Imports/Number of Glaciers/'\n",
    "\n",
    "fname = \"#glaciers-RGI-12.csv\"\n",
    "\n",
    "# Define the full path of the output file\n",
    "output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "number_glaciers.to_csv(output_path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93ef93-cd25-4e2d-804a-e7c24505b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_elements(list1, list2):\n",
    "    # Convert the lists to sets for efficient comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find elements that are in set1 but not in set2\n",
    "    unique_in_list1 = list(set1 - set2)\n",
    "\n",
    "    # Find elements that are in set2 but not in set1\n",
    "    unique_in_list2 = list(set2 - set1)\n",
    "\n",
    "    return unique_in_list1, unique_in_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea1fcf-102d-4a59-a626-a86598cfc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_glaciers = {}\n",
    "for b, basin in enumerate(Alpine_basins):\n",
    "    not_in_oggm, not_in_glo = find_unique_elements(glac_list['GloGEM'][basin], glac_list['OGGM'][basin])\n",
    "\n",
    "    not_in_py = find_unique_elements(glac_list['PyGEM'][basin], glac_list['OGGM'][basin])[1]\n",
    "    \n",
    "    # If both not_in_oggm and not_in_py are empty, set not_in_glo equal to not_in_oggm\n",
    "    if not not_in_oggm and not not_in_py:\n",
    "        not_in_py = not_in_oggm\n",
    "\n",
    "    # Store the results in the dictionary for this basin\n",
    "    missing_glaciers[basin] = {\n",
    "        'Basin': basin,\n",
    "        'GloGEM': not_in_glo,\n",
    "        'OGGM': not_in_oggm,\n",
    "        'PyGEM': not_in_py\n",
    "    }\n",
    "\n",
    "# Concatenate the dictionaries into a single DataFrame\n",
    "missing_glaciers_df = pd.DataFrame(missing_glaciers).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cc5b6-7068-485d-b0c0-b94efc059b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f840bbe-c1d8-460b-8584-72bcb82ff1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
